{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/pyparsing.py:3190: FutureWarning: Possible set intersection at position 3\n",
      "  self.re = re.compile(self.reString)\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import openml\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_datasets = [1590, 1486, 4541,4534,4135,40978,40670, 1111,42732, 42733]\n",
    "faulty_datasets = [23, 31, 188, 40996, 41161, 42734]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>openmlid</th>\n",
       "      <th>seed</th>\n",
       "      <th>algorithm</th>\n",
       "      <th>cpus</th>\n",
       "      <th>memory_max</th>\n",
       "      <th>time_created</th>\n",
       "      <th>host</th>\n",
       "      <th>executor</th>\n",
       "      <th>time_started</th>\n",
       "      <th>chosenmodel</th>\n",
       "      <th>errorrate</th>\n",
       "      <th>onlinedata</th>\n",
       "      <th>exception</th>\n",
       "      <th>time_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4031</td>\n",
       "      <td>1485</td>\n",
       "      <td>0</td>\n",
       "      <td>auto-sklearn</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-01-19 22:53:26</td>\n",
       "      <td>node10-033</td>\n",
       "      <td>oculus-7036263-128</td>\n",
       "      <td>2021-01-20 13:26:17</td>\n",
       "      <td>[(1.000000, SimpleClassificationPipeline({'bal...</td>\n",
       "      <td>0.088462</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-20 14:26:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4032</td>\n",
       "      <td>1485</td>\n",
       "      <td>1</td>\n",
       "      <td>auto-sklearn</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-01-19 22:53:26</td>\n",
       "      <td>node05-021</td>\n",
       "      <td>oculus-7036263-38</td>\n",
       "      <td>2021-01-20 13:26:16</td>\n",
       "      <td>[(1.000000, SimpleClassificationPipeline({'bal...</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-20 14:26:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4033</td>\n",
       "      <td>1590</td>\n",
       "      <td>0</td>\n",
       "      <td>auto-sklearn</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-01-19 22:53:26</td>\n",
       "      <td>node01-003</td>\n",
       "      <td>oculus-7118133-0</td>\n",
       "      <td>2021-02-22 17:54:54</td>\n",
       "      <td>[(1.000000, SimpleClassificationPipeline({'bal...</td>\n",
       "      <td>0.160524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-22 18:55:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4034</td>\n",
       "      <td>1590</td>\n",
       "      <td>1</td>\n",
       "      <td>auto-sklearn</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-01-19 22:53:26</td>\n",
       "      <td>node05-031</td>\n",
       "      <td>oculus-7118133-39</td>\n",
       "      <td>2021-02-22 17:54:55</td>\n",
       "      <td>[(1.000000, SimpleClassificationPipeline({'bal...</td>\n",
       "      <td>0.165029</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-02-22 18:55:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4035</td>\n",
       "      <td>1485</td>\n",
       "      <td>2</td>\n",
       "      <td>auto-sklearn</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-01-19 22:53:26</td>\n",
       "      <td>node08-047</td>\n",
       "      <td>oculus-7036263-66</td>\n",
       "      <td>2021-01-20 13:26:16</td>\n",
       "      <td>[(1.000000, SimpleClassificationPipeline({'bal...</td>\n",
       "      <td>0.069231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-20 14:26:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18079</th>\n",
       "      <td>25463</td>\n",
       "      <td>1067</td>\n",
       "      <td>9</td>\n",
       "      <td>naive-python-meta-noniterative-monotone</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-03-01 18:37:19</td>\n",
       "      <td>node11-030</td>\n",
       "      <td>oculus-7138810-6</td>\n",
       "      <td>2021-03-01 21:45:50</td>\n",
       "      <td>(None, None, &lt;class 'sklearn.ensemble._forest....</td>\n",
       "      <td>0.156398</td>\n",
       "      <td>{\"history\":[[702,0.19228070175438594],[784,0.1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-01 22:46:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18080</th>\n",
       "      <td>25464</td>\n",
       "      <td>1049</td>\n",
       "      <td>9</td>\n",
       "      <td>naive-python-meta-noniterative-monotone</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-03-01 18:37:19</td>\n",
       "      <td>node11-037</td>\n",
       "      <td>oculus-7138810-18</td>\n",
       "      <td>2021-03-01 21:42:09</td>\n",
       "      <td>(StandardScaler(), None, &lt;class 'sklearn.svm._...</td>\n",
       "      <td>0.095890</td>\n",
       "      <td>{\"history\":[[559,0.19949238578680206],[620,0.1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-03-01 22:33:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18081</th>\n",
       "      <td>25465</td>\n",
       "      <td>40975</td>\n",
       "      <td>8</td>\n",
       "      <td>naive-python-meta-noniterative-monotone</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-03-01 18:37:19</td>\n",
       "      <td>node12-043</td>\n",
       "      <td>oculus-7138810-106</td>\n",
       "      <td>2021-03-01 21:41:53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\tError class: java.lang.IllegalArgumentExce...</td>\n",
       "      <td>2021-03-01 21:41:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18082</th>\n",
       "      <td>25466</td>\n",
       "      <td>40975</td>\n",
       "      <td>9</td>\n",
       "      <td>naive-python-meta-noniterative</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-03-01 18:37:19</td>\n",
       "      <td>node10-032</td>\n",
       "      <td>oculus-7138810-149</td>\n",
       "      <td>2021-03-01 21:46:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\tError class: java.lang.IllegalArgumentExce...</td>\n",
       "      <td>2021-03-01 21:46:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18085</th>\n",
       "      <td>25469</td>\n",
       "      <td>40975</td>\n",
       "      <td>9</td>\n",
       "      <td>naive-python-meta-noniterative-monotone</td>\n",
       "      <td>8</td>\n",
       "      <td>24000</td>\n",
       "      <td>2021-03-01 18:37:19</td>\n",
       "      <td>node12-043</td>\n",
       "      <td>oculus-7138810-106</td>\n",
       "      <td>2021-03-01 21:41:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\tError class: java.lang.IllegalArgumentExce...</td>\n",
       "      <td>2021-03-01 21:41:54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16469 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       experiment_id  openmlid  seed                                algorithm  \\\n",
       "0               4031      1485     0                             auto-sklearn   \n",
       "1               4032      1485     1                             auto-sklearn   \n",
       "2               4033      1590     0                             auto-sklearn   \n",
       "3               4034      1590     1                             auto-sklearn   \n",
       "4               4035      1485     2                             auto-sklearn   \n",
       "...              ...       ...   ...                                      ...   \n",
       "18079          25463      1067     9  naive-python-meta-noniterative-monotone   \n",
       "18080          25464      1049     9  naive-python-meta-noniterative-monotone   \n",
       "18081          25465     40975     8  naive-python-meta-noniterative-monotone   \n",
       "18082          25466     40975     9           naive-python-meta-noniterative   \n",
       "18085          25469     40975     9  naive-python-meta-noniterative-monotone   \n",
       "\n",
       "       cpus  memory_max         time_created        host            executor  \\\n",
       "0         8       24000  2021-01-19 22:53:26  node10-033  oculus-7036263-128   \n",
       "1         8       24000  2021-01-19 22:53:26  node05-021   oculus-7036263-38   \n",
       "2         8       24000  2021-01-19 22:53:26  node01-003    oculus-7118133-0   \n",
       "3         8       24000  2021-01-19 22:53:26  node05-031   oculus-7118133-39   \n",
       "4         8       24000  2021-01-19 22:53:26  node08-047   oculus-7036263-66   \n",
       "...     ...         ...                  ...         ...                 ...   \n",
       "18079     8       24000  2021-03-01 18:37:19  node11-030    oculus-7138810-6   \n",
       "18080     8       24000  2021-03-01 18:37:19  node11-037   oculus-7138810-18   \n",
       "18081     8       24000  2021-03-01 18:37:19  node12-043  oculus-7138810-106   \n",
       "18082     8       24000  2021-03-01 18:37:19  node10-032  oculus-7138810-149   \n",
       "18085     8       24000  2021-03-01 18:37:19  node12-043  oculus-7138810-106   \n",
       "\n",
       "              time_started                                        chosenmodel  \\\n",
       "0      2021-01-20 13:26:17  [(1.000000, SimpleClassificationPipeline({'bal...   \n",
       "1      2021-01-20 13:26:16  [(1.000000, SimpleClassificationPipeline({'bal...   \n",
       "2      2021-02-22 17:54:54  [(1.000000, SimpleClassificationPipeline({'bal...   \n",
       "3      2021-02-22 17:54:55  [(1.000000, SimpleClassificationPipeline({'bal...   \n",
       "4      2021-01-20 13:26:16  [(1.000000, SimpleClassificationPipeline({'bal...   \n",
       "...                    ...                                                ...   \n",
       "18079  2021-03-01 21:45:50  (None, None, <class 'sklearn.ensemble._forest....   \n",
       "18080  2021-03-01 21:42:09  (StandardScaler(), None, <class 'sklearn.svm._...   \n",
       "18081  2021-03-01 21:41:53                                                NaN   \n",
       "18082  2021-03-01 21:46:59                                                NaN   \n",
       "18085  2021-03-01 21:41:54                                                NaN   \n",
       "\n",
       "       errorrate                                         onlinedata  \\\n",
       "0       0.088462                                                NaN   \n",
       "1       0.107692                                                NaN   \n",
       "2       0.160524                                                NaN   \n",
       "3       0.165029                                                NaN   \n",
       "4       0.069231                                                NaN   \n",
       "...          ...                                                ...   \n",
       "18079   0.156398  {\"history\":[[702,0.19228070175438594],[784,0.1...   \n",
       "18080   0.095890  {\"history\":[[559,0.19949238578680206],[620,0.1...   \n",
       "18081        NaN                                                NaN   \n",
       "18082        NaN                                                NaN   \n",
       "18085        NaN                                                NaN   \n",
       "\n",
       "                                               exception             time_end  \n",
       "0                                                    NaN  2021-01-20 14:26:32  \n",
       "1                                                    NaN  2021-01-20 14:26:44  \n",
       "2                                                    NaN  2021-02-22 18:55:20  \n",
       "3                                                    NaN  2021-02-22 18:55:23  \n",
       "4                                                    NaN  2021-01-20 14:26:40  \n",
       "...                                                  ...                  ...  \n",
       "18079                                                NaN  2021-03-01 22:46:30  \n",
       "18080                                                NaN  2021-03-01 22:33:13  \n",
       "18081  \\n\\tError class: java.lang.IllegalArgumentExce...  2021-03-01 21:41:54  \n",
       "18082  \\n\\tError class: java.lang.IllegalArgumentExce...  2021-03-01 21:46:59  \n",
       "18085  \\n\\tError class: java.lang.IllegalArgumentExce...  2021-03-01 21:41:54  \n",
       "\n",
       "[16469 rows x 15 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfResults = pd.read_csv(\"icml2021.csv\", delimiter=\";\")\n",
    "dfResults = dfResults[~dfResults[\"openmlid\"].isin(faulty_datasets)]\n",
    "#dfResults = dfResults[dfResults[\"errorrate\"].notna()]\n",
    "dfResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autosklearn_pipeline(desc):\n",
    "    pattern = \"SimpleClassificationPipeline(\"\n",
    "    pipeline_json = ast.literal_eval(desc[desc.index(pattern) + len(pattern):desc.index(\"\\n\")])[0]\n",
    "    return pipeline_json\n",
    "\n",
    "def uses_feature_preprocessing(pipeline):\n",
    "    for key in pipeline:\n",
    "        if \"feature_preprocessor:__choice__\" in key and not pipeline[key] in [\"no_preprocessing\", \"select_percentile_classification\"]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 10\n",
      "12 10\n",
      "54 10\n",
      "181 10\n",
      "1049 10\n",
      "1067 10\n",
      "1111 10\n",
      "1457 10\n",
      "1461 10\n",
      "1464 10\n",
      "1468 10\n",
      "1475 10\n",
      "1485 10\n",
      "1486 10\n",
      "1487 10\n",
      "1489 10\n",
      "1494 10\n",
      "1515 10\n",
      "1590 10\n",
      "4134 10\n",
      "4135 10\n",
      "4534 10\n",
      "4538 10\n",
      "4541 10\n",
      "23512 10\n",
      "23517 10\n",
      "40498 10\n",
      "40668 10\n",
      "40670 10\n",
      "40685 10\n",
      "40701 10\n",
      "40900 10\n",
      "40975 10\n",
      "40978 10\n",
      "40981 10\n",
      "40982 10\n",
      "40983 10\n",
      "40984 10\n",
      "41027 10\n",
      "41138 10\n",
      "41142 10\n",
      "41143 10\n",
      "41144 10\n",
      "41145 10\n",
      "41146 10\n",
      "41147 10\n",
      "41150 10\n",
      "41156 10\n",
      "41157 10\n",
      "41158 10\n",
      "41159 10\n",
      "41162 10\n",
      "41163 10\n",
      "41164 10\n",
      "41165 10\n",
      "41166 10\n",
      "41167 10\n",
      "41168 10\n",
      "41169 10\n",
      "42732 10\n",
      "42733 10\n"
     ]
    }
   ],
   "source": [
    "for openmlid, dfDataset in dfResults[dfResults[\"algorithm\"].str.contains(\"auto-weka\")].groupby(\"openmlid\"):\n",
    "    print(openmlid, len(dfDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getResultTable(dfResults, rowAttribute, colAttribute, resultAttribute, col_labels = None, cellFormatter = None, comparativeFormatters = None, row_formatter = None):\n",
    "    rows = []\n",
    "    colnames = list(pd.unique(dfResults[colAttribute]))\n",
    "    for rowIndex, dfRow in dfResults.groupby(rowAttribute):\n",
    "        \n",
    "        # identify all scores for row\n",
    "        scores = {}\n",
    "        for colName in colnames:\n",
    "            if colName != rowAttribute:\n",
    "                scoresForCell = dfRow[(dfRow[colAttribute] == colName) & (dfRow[resultAttribute].notna())][resultAttribute].values\n",
    "                if len(scoresForCell) == 0:\n",
    "                    scoresForCell = [np.nan]\n",
    "                scores[colName] = sorted(scoresForCell)\n",
    "        \n",
    "        rows.append([rowIndex] + row_formatter(rowIndex, scores))\n",
    "    \n",
    "    with pd.option_context(\"max_colwidth\", 1000):\n",
    "        return pd.DataFrame(rows, columns=[rowAttribute] + [col_labels[col] if not col_labels is None and col in col_labels else col for col in colnames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2958: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2972: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2958: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2972: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{r||rr|rr||r|rr}\n",
      "\\toprule\n",
      " & \\multicolumn{4}{c||}{WEKA backend} & \\multicolumn{3}{c}{scikit-learn backend}\\\\\n",
      " \\multicolumn{1}{c}{id} &                                  \\multicolumn{1}{c}{auto-weka} &                                     \\multicolumn{1}{c}{mlplan} & \\multicolumn{1}{c}{primitive} &   \\multicolumn{1}{c}{full} &                                   \\multicolumn{1}{c}{asklearn} & \\multicolumn{1}{c}{primitive} &   \\multicolumn{1}{c}{full} \\\\\n",
      "\\midrule\n",
      "                      3 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.01$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "                     12 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.04$\\pm$0.01 \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.02$\\pm$0.02} &  \\underline{0.03$\\pm$0.02} \\\\\n",
      "                     54 &  \\underline{0.18$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.18$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.17$\\pm$0.02} &  \\underline{0.18$\\pm$0.05} &                                  0.19$\\pm$0.03 $\\circ$ $\\circ$ &        \\textbf{0.16$\\pm$0.03} &  \\underline{0.17$\\pm$0.03} \\\\\n",
      "                    181 &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.38$\\pm$0.03} &  \\underline{0.39$\\pm$0.04} &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.38$\\pm$0.03} &     \\textbf{0.37$\\pm$0.05} \\\\\n",
      "                   1049 &   \\underline{0.1$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.09$\\pm$0.02} &     \\textbf{0.09$\\pm$0.02} &     \\textbf{0.09$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.09$\\pm$0.02} &     \\textbf{0.09$\\pm$0.01} \\\\\n",
      "                   1067 &     \\textbf{0.15$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.15$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.15$\\pm$0.02} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.14$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.15$\\pm$0.02} &  \\underline{0.15$\\pm$0.01} \\\\\n",
      "                   1111 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &              \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} $\\bullet$ &         \\textbf{0.02$\\pm$0.0} &               0.04$\\pm$0.0 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} \\\\\n",
      "                   1457 &   \\underline{0.29$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.25$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.25$\\pm$0.03} &     \\textbf{0.24$\\pm$0.03} &                                 0.2$\\pm$0.02 $\\bullet$ $\\circ$ &                 0.23$\\pm$0.04 &     \\textbf{0.16$\\pm$0.03} \\\\\n",
      "                   1461 &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.1$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &      \\textbf{0.11$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.11$\\pm$0.0} &      \\textbf{0.11$\\pm$0.0} \\\\\n",
      "                   1464 &  \\underline{0.22$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.24$\\pm$0.01 $\\circ$ \\phantom{$\\circ$} &        \\textbf{0.21$\\pm$0.04} &              0.23$\\pm$0.03 &  \\underline{0.22$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.22$\\pm$0.02} &     \\textbf{0.21$\\pm$0.03} \\\\\n",
      "                   1468 &  \\underline{0.06$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.04$\\pm$0.02} &     \\textbf{0.04$\\pm$0.01} \\\\\n",
      "                   1475 &              0.38$\\pm$0.03 \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.36$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.37$\\pm$0.02} &  \\underline{0.37$\\pm$0.02} &                                  0.39$\\pm$0.03 $\\circ$ $\\circ$ &        \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} \\\\\n",
      "                   1485 &                                   0.39$\\pm$0.1 $\\circ$ $\\circ$ &                        0.23$\\pm$0.05 \\phantom{$\\circ$} $\\circ$ &                 0.23$\\pm$0.02 &     \\textbf{0.11$\\pm$0.02} &             \\textbf{0.11$\\pm$0.02} $\\dagger$ \\phantom{$\\circ$} &                 0.26$\\pm$0.03 &  \\underline{0.12$\\pm$0.01} \\\\\n",
      "                   1486 &      \\textbf{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                  0.05$\\pm$0.01 $\\circ$ $\\circ$ &         \\textbf{0.03$\\pm$0.0} &      \\textbf{0.03$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} \\\\\n",
      "                   1487 &     \\textbf{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.06$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} \\\\\n",
      "                   1489 &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &   \\underline{0.1$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} \\\\\n",
      "                   1494 &  \\underline{0.13$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.13$\\pm$0.02} &     \\textbf{0.12$\\pm$0.03} &                        0.14$\\pm$0.03 \\phantom{$\\circ$} $\\circ$ &     \\underline{0.13$\\pm$0.03} &     \\textbf{0.12$\\pm$0.03} \\\\\n",
      "                   1515 &     \\textbf{0.09$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.13$\\pm$0.04} &  \\underline{0.13$\\pm$0.04} &             \\textbf{0.07$\\pm$0.03} $\\bullet$ \\phantom{$\\circ$} &     \\underline{0.13$\\pm$0.03} &     \\textbf{0.07$\\pm$0.26} \\\\\n",
      "                   1590 &      \\textbf{0.14$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.14$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.14$\\pm$0.0} &     \\textbf{0.14$\\pm$0.01} &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.16$\\pm$0.0} &      \\textbf{0.16$\\pm$0.0} \\\\\n",
      "                   4134 &  \\underline{0.23$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.2$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.2$\\pm$0.01} &  \\underline{0.22$\\pm$0.05} &   \\underline{0.2$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.19$\\pm$0.02} &   \\underline{0.2$\\pm$0.02} \\\\\n",
      "                   4135 &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.06$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.05$\\pm$0.0} &              0.08$\\pm$0.03 &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} \\\\\n",
      "                   4534 &   \\underline{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                  0.04$\\pm$0.01 $\\circ$ $\\circ$ &         \\textbf{0.02$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.02$\\pm$0.01} &      \\textbf{0.02$\\pm$0.0} \\\\\n",
      "                   4538 &  \\underline{0.32$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.32$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.32$\\pm$0.01} &     \\textbf{0.31$\\pm$0.01} &  \\underline{0.31$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} \\\\\n",
      "                   4541 &   \\underline{0.43$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.42$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.42$\\pm$0.01} &     \\textbf{0.42$\\pm$0.01} &      \\textbf{0.42$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.42$\\pm$0.0} &      \\textbf{0.42$\\pm$0.0} \\\\\n",
      "                  23512 &                         0.31$\\pm$0.0 \\phantom{$\\circ$} $\\circ$ &                        0.32$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ &                 0.31$\\pm$0.02 &     \\textbf{0.28$\\pm$0.01} &      \\textbf{0.28$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.28$\\pm$0.01} &     \\textbf{0.28$\\pm$0.01} \\\\\n",
      "                  23517 &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.48$\\pm$0.0} &     \\textbf{0.48$\\pm$0.01} &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.48$\\pm$0.0} &      \\textbf{0.48$\\pm$0.0} \\\\\n",
      "                  40498 &  \\underline{0.31$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.31$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &                                  0.34$\\pm$0.02 $\\circ$ $\\circ$ &        \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} \\\\\n",
      "                  40668 &              \\textbf{0.03$\\pm$0.0} \\phantom{$\\circ$} $\\bullet$ &                      0.19$\\pm$0.01 \\phantom{$\\circ$} $\\bullet$ &     \\underline{0.21$\\pm$0.11} &               0.27$\\pm$0.0 &                                0.17$\\pm$0.06 $\\dagger$ $\\circ$ &                  0.26$\\pm$0.0 &     \\textbf{0.03$\\pm$0.01} \\\\\n",
      "                  40670 &     \\textbf{0.04$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.02} &     \\textbf{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} \\\\\n",
      "                  40685 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "                  40701 &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.04$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &      \\textbf{0.04$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} \\\\\n",
      "                  40900 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "                  40975 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.0$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.01$\\pm$0.01} &  \\underline{0.01$\\pm$0.01} \\\\\n",
      "                  40978 &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.02$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} \\\\\n",
      "                  40981 &                        0.14$\\pm$0.05 $\\circ$ \\phantom{$\\circ$} &                        0.14$\\pm$0.03 $\\circ$ \\phantom{$\\circ$} &        \\textbf{0.12$\\pm$0.03} &  \\underline{0.13$\\pm$0.03} &     \\textbf{0.12$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.12$\\pm$0.03} &     \\textbf{0.12$\\pm$0.02} \\\\\n",
      "                  40982 &     \\textbf{0.19$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.2$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\underline{0.2$\\pm$0.03} &  \\underline{0.21$\\pm$0.03} &             \\textbf{0.21$\\pm$0.04} \\phantom{$\\circ$} $\\dagger$ &        \\textbf{0.21$\\pm$0.02} &               0.25$\\pm$0.1 \\\\\n",
      "                  40983 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.02$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "                  40984 &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                  0.04$\\pm$0.01 $\\circ$ $\\circ$ &        \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "                  41027 &                                  0.16$\\pm$0.02 $\\circ$ $\\circ$ &                                  0.15$\\pm$0.02 $\\circ$ $\\circ$ &         \\textbf{0.09$\\pm$0.0} &     \\textbf{0.09$\\pm$0.02} &                     \\textbf{0.07$\\pm$0.01} $\\dagger$ $\\dagger$ &                 0.16$\\pm$0.01 &               0.14$\\pm$0.0 \\\\\n",
      "                  41138 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "                  41142 &                                  0.29$\\pm$0.02 $\\circ$ $\\circ$ &                        0.28$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ &     \\underline{0.27$\\pm$0.02} &     \\textbf{0.26$\\pm$0.03} &     \\textbf{0.27$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.28$\\pm$0.02} &  \\underline{0.28$\\pm$0.02} \\\\\n",
      "                  41143 &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.17$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.17$\\pm$0.02} &  \\underline{0.18$\\pm$0.02} &     \\textbf{0.18$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.19$\\pm$0.02} &   \\underline{0.2$\\pm$0.04} \\\\\n",
      "                  41144 &  \\underline{0.27$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.19$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ &                 0.21$\\pm$0.05 &     \\textbf{0.09$\\pm$0.01} &              \\textbf{0.1$\\pm$0.02} $\\dagger$ \\phantom{$\\circ$} &                 0.24$\\pm$0.02 &  \\underline{0.11$\\pm$0.02} \\\\\n",
      "                  41145 &                                  0.29$\\pm$0.03 $\\circ$ $\\circ$ &                        0.26$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ &                 0.25$\\pm$0.01 &     \\textbf{0.16$\\pm$0.03} &                     \\textbf{0.18$\\pm$0.01} $\\dagger$ $\\dagger$ &                 0.25$\\pm$0.02 &              0.24$\\pm$0.01 \\\\\n",
      "                  41146 &      \\textbf{0.04$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                  0.07$\\pm$0.01 $\\circ$ $\\circ$ &     \\underline{0.05$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.05$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} \\\\\n",
      "                  41147 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.32$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.33$\\pm$0.01} &      \\textbf{0.32$\\pm$0.0} &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.32$\\pm$0.0} &                        nan \\\\\n",
      "                  41150 &      \\textbf{0.07$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.08$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\underline{0.09$\\pm$0.0} &  \\underline{0.08$\\pm$0.01} &      \\textbf{0.06$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\underline{0.07$\\pm$0.0} &      \\textbf{0.06$\\pm$0.0} \\\\\n",
      "                  41156 &     \\textbf{0.15$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.15$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.15$\\pm$0.01} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.14$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.14$\\pm$0.01} &     \\textbf{0.14$\\pm$0.01} \\\\\n",
      "                  41157 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.21$\\pm$0.14 $\\circ$ \\phantom{$\\circ$} &        \\textbf{0.15$\\pm$0.09} &              0.22$\\pm$0.11 &                        0.24$\\pm$0.09 $\\circ$ \\phantom{$\\circ$} &        \\textbf{0.11$\\pm$0.09} &              0.26$\\pm$0.15 \\\\\n",
      "                  41158 &             \\textbf{0.07$\\pm$0.01} \\phantom{$\\circ$} $\\bullet$ &             \\textbf{0.07$\\pm$0.01} \\phantom{$\\circ$} $\\bullet$ &        \\textbf{0.07$\\pm$0.04} &              0.19$\\pm$0.05 &     \\textbf{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} \\\\\n",
      "                  41159 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.2$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &                 0.21$\\pm$0.01 &     \\textbf{0.18$\\pm$0.03} &     \\textbf{0.17$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\underline{0.18$\\pm$0.01} &                        nan \\\\\n",
      "                  41162 &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &               \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} $\\bullet$ &          \\textbf{0.1$\\pm$0.0} &  \\underline{0.22$\\pm$0.06} &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} \\\\\n",
      "                  41163 &   \\underline{0.08$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\underline{0.03$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} \\\\\n",
      "                  41164 &                                  0.35$\\pm$0.01 $\\circ$ $\\circ$ &                        0.32$\\pm$0.02 $\\circ$ \\phantom{$\\circ$} &         \\textbf{0.3$\\pm$0.01} &              0.33$\\pm$0.02 &                                  0.31$\\pm$0.02 $\\circ$ $\\circ$ &        \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} \\\\\n",
      "                  41165 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.62$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.6$\\pm$0.03} &                        nan &     \\textbf{0.55$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} &        \\textbf{0.55$\\pm$0.01} &  \\underline{0.56$\\pm$0.01} \\\\\n",
      "                  41166 &  \\underline{0.36$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.35$\\pm$0.01 $\\circ$ \\phantom{$\\circ$} &         \\textbf{0.3$\\pm$0.01} &               0.34$\\pm$0.1 &                      \\textbf{0.28$\\pm$0.0} $\\dagger$ $\\dagger$ &                  0.33$\\pm$0.0 &               0.32$\\pm$0.0 \\\\\n",
      "                  41167 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} &                                   0.96$\\pm$0.0 $\\circ$ $\\circ$ &                 0.12$\\pm$0.07 &      \\textbf{0.09$\\pm$0.0} &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.31$\\pm$0.0} &                        nan \\\\\n",
      "                  41168 &  \\underline{0.34$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} &            \\underline{0.3$\\pm$0.0} $\\bullet$ \\phantom{$\\circ$} &                 0.34$\\pm$0.01 &     \\textbf{0.29$\\pm$0.01} &                      \\textbf{0.27$\\pm$0.0} $\\bullet$ $\\bullet$ &                 0.29$\\pm$0.01 &               0.29$\\pm$0.0 \\\\\n",
      "                  41169 &      \\textbf{0.71$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &           \\underline{0.72$\\pm$0.0} \\phantom{$\\circ$} $\\bullet$ &      \\underline{0.72$\\pm$0.0} &  \\underline{0.75$\\pm$0.01} &                                  0.69$\\pm$0.01 $\\circ$ $\\circ$ &      \\underline{0.65$\\pm$0.0} &      \\textbf{0.64$\\pm$0.0} \\\\\n",
      "                  42732 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.12$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} \\\\\n",
      "                  42733 &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\underline{0.17$\\pm$0.02} \\phantom{$\\circ$} $\\bullet$ &      \\underline{0.17$\\pm$0.0} &  \\underline{0.25$\\pm$0.03} &      \\textbf{0.17$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} &         \\textbf{0.17$\\pm$0.0} &      \\textbf{0.17$\\pm$0.0} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_default_best_formatting(scores, indices):\n",
    "    cellFormatter = lambda x: str(np.round(scipy.stats.trim_mean(x, 0.1), 2)) + \"$\\pm$\" + str(np.round(np.nanstd(x), 2)) if len([y for y in x if not np.isnan(y)]) > 0 else \"nan\"\n",
    "    \n",
    "    # get best score (and column name)\n",
    "    bestMean = None\n",
    "    bestCol = None\n",
    "    for colName in scores:\n",
    "        scoresForCell = scores[colName]\n",
    "        meanScore = scipy.stats.trim_mean(scoresForCell, 0.1)\n",
    "        if ~np.isnan(meanScore) and (bestCol is None or bestMean > meanScore):\n",
    "            bestMean = meanScore\n",
    "            bestCol = colName\n",
    "    \n",
    "    # now apply formatters to cells and add them to row\n",
    "    bestScores = scores[bestCol] if not bestCol is None else []\n",
    "    \n",
    "    # formatting of best cells\n",
    "    row = []\n",
    "    if bestMean is None:\n",
    "        for colName in indices:\n",
    "            row.append(\"nan\")\n",
    "    else:\n",
    "        for colName in indices:\n",
    "            scoresForCell = scores[colName]\n",
    "            curMean = scipy.stats.trim_mean(scoresForCell, 0.1)\n",
    "            isBest = ((bestCol == colName) | (np.round(curMean, 2) == np.round(bestMean, 2)))\n",
    "            if not isBest:\n",
    "                n = min(len(bestScores), len(scoresForCell))\n",
    "                if n > 0:\n",
    "                    if n > 1:\n",
    "                        wilcoxon = sp.stats.wilcoxon(bestScores[:n], scoresForCell[:n])\n",
    "                        isSignificant = wilcoxon.pvalue < 0.05\n",
    "                        isRelevant = np.round(np.abs(np.round(bestMean, 2) - np.round(curMean, 2)), 2) > 0.01\n",
    "                    else:\n",
    "                        isSignificant = False\n",
    "                        isRelevant = False\n",
    "                else:\n",
    "                    isSignificant = True\n",
    "                    isRelevant = True\n",
    "            entry = cellFormatter(scoresForCell)\n",
    "            if isBest:\n",
    "                entry = \"\\\\textbf{\" + str(entry) + \"}\"\n",
    "            elif entry != \"nan\" and not (isSignificant and isRelevant):\n",
    "                entry = \"\\\\underline{\" + str(entry) + \"}\"\n",
    "            row.append(entry)\n",
    "    return row\n",
    "\n",
    "def row_layouter(openmlid, scores):\n",
    "    \n",
    "    indices = [x for x in scores]\n",
    "    row = get_default_best_formatting(scores, indices)\n",
    "    \n",
    "    # we now have one (formatted) entry for each column. Now add bullet/circ symbols\n",
    "    naive_cols = [x for x in scores if \"naive\" in x]\n",
    "    competitor_cols = [x for x in scores if not \"naive\" in x]\n",
    "    for naive_col in naive_cols:\n",
    "        scores_naive = scores[naive_col]\n",
    "        mean_naive = np.round(scipy.stats.trim_mean(scores_naive, 0.1), 2)\n",
    "        for competitor_col in competitor_cols:\n",
    "            scores_competitor = scores[competitor_col]\n",
    "            mean_competitor = np.round(scipy.stats.trim_mean(scores_competitor, 0.1), 2)\n",
    "            n = min(len(scores_naive), len(scores_competitor))\n",
    "            if n > 1 and scores_naive[:n] != scores_competitor[:n]:\n",
    "                wilcoxon = sp.stats.wilcoxon(scores_naive[:n], scores_competitor[:n])\n",
    "                isSignificant = wilcoxon.pvalue < 0.05\n",
    "            else:\n",
    "                isSignificant = False\n",
    "            \n",
    "            is_substantially_better = np.round(mean_naive - mean_competitor, 2) > 0.01 and isSignificant\n",
    "            is_substantially_worse = np.round(mean_competitor - mean_naive, 2) > 0.01 and isSignificant\n",
    "            i = indices.index(competitor_col)\n",
    "            if is_substantially_worse:\n",
    "                row[i] += \" $\\\\circ$\"\n",
    "            elif is_substantially_better:\n",
    "                if competitor_col == \"auto-sklearn\":\n",
    "                    models = [get_autosklearn_pipeline(desc) for desc in dfResults[(dfResults[\"algorithm\"] == \"auto-sklearn\") & (dfResults[\"openmlid\"] == openmlid)][\"chosenmodel\"].values]\n",
    "                    used_preprocessing = np.round(np.mean([uses_feature_preprocessing(mod) for mod in models]))\n",
    "                    if used_preprocessing:\n",
    "                        row[i] += \" $\\\\dagger$\"\n",
    "                    else:\n",
    "                        row[i] += \" $\\\\bullet$\"\n",
    "                else:\n",
    "                    row[i] += \" $\\\\bullet$\"\n",
    "                \n",
    "            else:\n",
    "                row[i] += \" \\\\phantom{$\\\\circ$}\"\n",
    "        \n",
    "    return row\n",
    "\n",
    "\n",
    "filter_python = dfResults[\"algorithm\"] == \"auto-sklearn\"\n",
    "filter_python = filter_python | ((dfResults[\"algorithm\"].str.contains(\"python\")) & (dfResults[\"algorithm\"].str.contains(\"primitive\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")))\n",
    "filter_python = filter_python | ((dfResults[\"algorithm\"].str.contains(\"python\")) & (dfResults[\"algorithm\"].str.contains(\"validation\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")) & dfResults[\"algorithm\"].str.contains(\"monotone\"))\n",
    "\n",
    "filter_java = dfResults[\"algorithm\"] == \"auto-weka\"\n",
    "filter_java = filter_java | (dfResults[\"algorithm\"] == \"mlplan\")\n",
    "filter_java = filter_java | ((dfResults[\"algorithm\"].str.contains(\"java\")) & (dfResults[\"algorithm\"].str.contains(\"primitive\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")))\n",
    "filter_java = filter_java | ((dfResults[\"algorithm\"].str.contains(\"java\")) & (dfResults[\"algorithm\"].str.contains(\"validation\")) & (dfResults[\"algorithm\"].str.contains(\"monotone\")))\n",
    "\n",
    "col_labels_python = {\n",
    "    \"auto-sklearn\": \"asklearn\",\n",
    "    \"naive-python-primitive-noniterative\": \"primitive\",\n",
    "    \"naive-python-scaling-noniterative\": \"scaling\",\n",
    "    \"naive-python-filtering-noniterative\": \"filtering\",\n",
    "    \"naive-python-filtering-noniterative-monotone\": \"filtering\",\n",
    "    \"naive-python-meta-noniterative\": \"meta\",\n",
    "    \"naive-python-meta-noniterative-monotone\": \"meta\",\n",
    "    \"naive-python-tuning-noniterative\": \"tuning\",\n",
    "    \"naive-python-tuning-noniterative-monotone\": \"tuning\",\n",
    "    \"naive-python-validation-noniterative\": \"validation\",\n",
    "    \"naive-python-validation-noniterative-monotone\": \"full\",\n",
    "    \"naive-java-primitive-noniterative\": \"primitive\",\n",
    "    \"naive-java-filtering-noniterative\": \"filtering\",\n",
    "    \"naive-java-filtering-noniterative-monotone\": \"filtering\",\n",
    "    \"naive-java-meta-noniterative\": \"meta\",\n",
    "    \"naive-java-meta-noniterative-monotone\": \"meta\",\n",
    "    \"naive-java-tuning-noniterative\": \"tuning\",\n",
    "    \"naive-java-tuning-noniterative-monotone\": \"tuning\",\n",
    "    \"naive-java-validation-noniterative-monotone\": \"full\",\n",
    "    \"naive-java-primitive-noniterative\": \"primitive\",\n",
    "    \"naive-java-validation-noniterative\": \"validation\",\n",
    "    \"naive-java-validation-noniterative-monotone\": \"full\",\n",
    "}\n",
    "\n",
    "\n",
    "dfResultsFinalJava = getResultTable(dfResults[filter_java], \"openmlid\", \"algorithm\", \"errorrate\", col_labels=col_labels_python, row_formatter=row_layouter)[[\"openmlid\", \"auto-weka\", \"mlplan\", \"primitive\", \"full\"]]\n",
    "\n",
    "dfResultsFinalPython = getResultTable(dfResults[filter_python], \"openmlid\", \"algorithm\", \"errorrate\", col_labels=col_labels_python, row_formatter=row_layouter)\n",
    "\n",
    "latex_colname_map = {\n",
    "    \"primitive_x\": \"primitive\",\n",
    "    \"primitive_y\": \"primitive\",\n",
    "    \"full_x\": \"full\",\n",
    "    \"full_y\": \"full\",\n",
    "    \"openmlid\": \"id\"\n",
    "}\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    dfMerge = dfResultsFinalJava.merge(dfResultsFinalPython, on=\"openmlid\").rename(columns=latex_colname_map)\n",
    "    latex_colname_map_centered = {}\n",
    "    for col in dfMerge.columns:\n",
    "        latex_colname_map_centered[col] = \"\\\\multicolumn{1}{c}{\" + col + \"}\"\n",
    "    dfMerge = dfMerge.rename(columns=latex_colname_map_centered)\n",
    "    str_base_table = dfMerge.to_latex(index=False, escape=False)\n",
    "    print(str_base_table.replace(\"{rlllllll}\\n\\\\toprule\", \"{r||rr|rr||r|rr}\\n\\\\toprule\\n & \\multicolumn{4}{c||}{WEKA backend} & \\multicolumn{3}{c}{scikit-learn backend}\\\\\\\\\"))\n",
    "#dfResultsFinalJava.iloc[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l|rrrr||rrrrr}\n",
      "\\toprule\n",
      " & \\multicolumn{4}{c||}{WEKA backend} & \\multicolumn{5}{c}{scikit-learn backend}\\\\\n",
      "{} &  filtering &  meta &  tuning &  validation &  scaling &  filtering &  meta &  tuning &  validation \\\\\n",
      "\\midrule\n",
      "wins        &          6 &     4 &       1 &           4 &        7 &          2 &     2 &       3 &           1 \\\\\n",
      "unique wins &          5 &     1 &       0 &           3 &        7 &          2 &     0 &       0 &           1 \\\\\n",
      "losses      &          2 &     5 &       4 &           4 &        1 &          1 &     1 &       2 &           1 \\\\\n",
      "draws       &         53 &    52 &      56 &          53 &       53 &         58 &    58 &      56 &          59 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getGameTable4Isolated(dfResults, primitive_column, col_labels):\n",
    "    algorithms = [a for a in pd.unique(dfResults[\"algorithm\"]) if a != primitive_column and not \"monotone\" in a]\n",
    "    wins = {a: 0 for a in algorithms}\n",
    "    wins_unique = {a: 0 for a in algorithms}\n",
    "    losses = {a: 0 for a in algorithms}\n",
    "    draws = {a: 0 for a in algorithms}\n",
    "    for openmlid, dfReduced in dfResults.groupby(\"openmlid\"):\n",
    "        \n",
    "        base_performance = scipy.stats.trim_mean(dfReduced[dfReduced[\"algorithm\"] == primitive_column][\"errorrate\"].values, 0.1)\n",
    "        for algorithm in algorithms:\n",
    "            performance = scipy.stats.trim_mean(dfReduced[dfReduced[\"algorithm\"] == algorithm][\"errorrate\"].values, 0.1)\n",
    "            if performance <= base_performance - 0.01:\n",
    "                wins[algorithm] += 1\n",
    "                \n",
    "                best_performance_of_other = min([scipy.stats.trim_mean(dfReduced[dfReduced[\"algorithm\"] == a][\"errorrate\"].values, 0.1) for a in algorithms if a != algorithm])\n",
    "                if best_performance_of_other > performance:\n",
    "                    wins_unique[algorithm] += 1\n",
    "                \n",
    "            elif performance - 0.01 >= base_performance:\n",
    "                losses[algorithm] += 1\n",
    "            else:\n",
    "                draws[algorithm] += 1\n",
    "    \n",
    "    # create data frame\n",
    "    cols = []\n",
    "    for algo in algorithms:\n",
    "        cols.append([wins[algo], wins_unique[algo], losses[algo], draws[algo]])\n",
    "    return pd.DataFrame(np.array(cols).T, columns=[col_labels[a] if a in col_labels else a for a in algorithms], index=[\"wins\", \"unique wins\", \"losses\", \"draws\"])\n",
    "    \n",
    "filter_java = (dfResults[\"algorithm\"].str.contains(\"java\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\") & (~dfResults[\"algorithm\"].str.contains(\"monotone\")))\n",
    "\n",
    "filter_python = (dfResults[\"algorithm\"].str.contains(\"python\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\") & (~dfResults[\"algorithm\"].str.contains(\"monotone\")))\n",
    "filter_python = filter_python & ~dfResults[\"algorithm\"].str.contains(\"wrapping\")\n",
    "col_labels = {\n",
    "    \"naive-python-primitive-noniterative\": \"primitive\",\n",
    "    \"naive-python-scaling-noniterative\": \"scaling\",\n",
    "    \"naive-python-filtering-noniterative\": \"filtering\",\n",
    "    \"naive-python-meta-noniterative\": \"meta\",\n",
    "    \"naive-python-tuning-noniterative\": \"tuning\",\n",
    "    \"naive-python-validation-noniterative\": \"validation\",\n",
    "    \"naive-java-primitive-noniterative\": \"primitive\",\n",
    "    \"naive-java-scaling-noniterative\": \"scaling\",\n",
    "    \"naive-java-filtering-noniterative\": \"filtering\",\n",
    "    \"naive-java-meta-noniterative\": \"meta\",\n",
    "    \"naive-java-tuning-noniterative\": \"tuning\",\n",
    "    \"naive-java-validation-noniterative\": \"validation\"\n",
    "}\n",
    "\n",
    "dfTournamentPython = getGameTable4Isolated(dfResults[filter_python], primitive_column=\"naive-python-primitive-noniterative\", col_labels = col_labels)[[\"scaling\", \"filtering\", \"meta\", \"tuning\", \"validation\"]]\n",
    "dfTournamentJava = getGameTable4Isolated(dfResults[filter_java], primitive_column=\"naive-java-primitive-noniterative\", col_labels = col_labels)[[\"filtering\", \"meta\", \"tuning\", \"validation\"]]\n",
    "tournament_latex = pd.concat([dfTournamentJava, dfTournamentPython], axis=1).to_latex()\n",
    "print(tournament_latex.replace(\"{lrrrrrrrrr}\\n\\\\toprule\", \"{l|rrrr||rrrrr}\\n\\\\toprule\\n & \\multicolumn{4}{c||}{WEKA backend} & \\multicolumn{5}{c}{scikit-learn backend}\\\\\\\\\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monotone Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synergentic Gains Due to Combined Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for naive-python-validation-noniterative-monotone on12\n",
      "Loss for naive-python-tuning-noniterative-monotone on54\n",
      "Loss for naive-python-validation-noniterative-monotone on54\n",
      "Loss for naive-python-tuning-noniterative-monotone on1049\n",
      "Loss for naive-python-filtering-noniterative-monotone on1067\n",
      "Loss for naive-python-validation-noniterative-monotone on1457\n",
      "Loss for naive-python-meta-noniterative-monotone on1468\n",
      "Loss for naive-python-meta-noniterative-monotone on1515\n",
      "Loss for naive-python-tuning-noniterative-monotone on4134\n",
      "Loss for naive-python-tuning-noniterative-monotone on4538\n",
      "Loss for naive-python-validation-noniterative-monotone on4538\n",
      "Loss for naive-python-validation-noniterative-monotone on40981\n",
      "Loss for naive-python-validation-noniterative-monotone on40982\n",
      "Loss for naive-python-tuning-noniterative-monotone on40984\n",
      "Loss for naive-python-filtering-noniterative-monotone on41157\n",
      "Loss for naive-python-tuning-noniterative-monotone on41157\n",
      "Loss for naive-python-validation-noniterative-monotone on41157\n",
      "Loss for naive-python-meta-noniterative-monotone on41157\n",
      "Loss for naive-python-tuning-noniterative-monotone on41158\n",
      "Loss for naive-python-meta-noniterative-monotone on41158\n",
      "Loss for naive-python-validation-noniterative-monotone on41165\n",
      "\\begin{tabular}{l||rrr|rrrr}\n",
      "\\toprule\n",
      " & \\multicolumn{3}{c|}{WEKA backend} & \\multicolumn{3}{c}{scikit-learn backend}\\\\\n",
      "{} &  meta &  tuning &  validation &  filtering &  meta &  tuning &  validation \\\\\n",
      "\\midrule\n",
      "wins   &     0 &       0 &           0 &          0 &     2 &       0 &           0 \\\\\n",
      "losses &     0 &       0 &           0 &          2 &     4 &       7 &           8 \\\\\n",
      "draws  &    61 &      61 &          61 &         59 &    55 &      54 &          53 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getSynnergyTable(dfResults, col_labels):\n",
    "    algorithms = [a for a in pd.unique(dfResults[\"algorithm\"]) if \"monotone\" in a]\n",
    "    wins = {a: 0 for a in algorithms}\n",
    "    losses = {a: 0 for a in algorithms}\n",
    "    draws = {a: 0 for a in algorithms}\n",
    "    for openmlid, dfReduced in dfResults.groupby(\"openmlid\"):\n",
    "        for algorithm in algorithms:\n",
    "            \n",
    "            base_algorithms = [\"naive-python-primitive-noniterative\", \"naive-python-scaling-noniterative\", algorithm.replace(\"-monotone\", \"\")]\n",
    "            if \"naive-python-validation-noniterative\" in base_algorithms:\n",
    "                base_algorithms.append(\"naive-python-tuning-noniterative\")\n",
    "            if \"naive-python-tuning-noniterative\" in base_algorithms:\n",
    "                base_algorithms.append(\"naive-python-filtering-noniterative\")\n",
    "            \n",
    "            \n",
    "            base_performances = []            \n",
    "            for base_algorithm in base_algorithms:\n",
    "                base_performances.append(scipy.stats.trim_mean(dfReduced[dfReduced[\"algorithm\"] == base_algorithm][\"errorrate\"].values, 0.1))\n",
    "            base_performance = min(base_performances)\n",
    "            \n",
    "            performance = scipy.stats.trim_mean(dfReduced[dfReduced[\"algorithm\"] == algorithm][\"errorrate\"].values, 0.1)\n",
    "            if performance <= base_performance - 0.01:\n",
    "                wins[algorithm] += 1\n",
    "            elif performance - 0.01 >= base_performance:\n",
    "                losses[algorithm] += 1\n",
    "                print(\"Loss for \" + algorithm + \" on\" + str(openmlid))\n",
    "            else:\n",
    "                draws[algorithm] += 1\n",
    "    \n",
    "    # create data frame\n",
    "    cols = []\n",
    "    for algo in algorithms:\n",
    "        cols.append([wins[algo], losses[algo], draws[algo]])\n",
    "    return pd.DataFrame(np.array(cols).T, columns=[col_labels[a] if a in col_labels else a for a in algorithms], index=[\"wins\", \"losses\", \"draws\"])\n",
    "    \n",
    "filter_python = ((dfResults[\"algorithm\"].str.contains(\"python\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")))\n",
    "filter_python = filter_python & ~dfResults[\"algorithm\"].str.contains(\"wrapping\")\n",
    "filter_java = ((dfResults[\"algorithm\"].str.contains(\"java\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")))\n",
    "col_labels = {\n",
    "    \"naive-python-filtering-noniterative-monotone\": \"filtering\",\n",
    "    \"naive-python-meta-noniterative-monotone\": \"meta\",\n",
    "    \"naive-python-tuning-noniterative-monotone\": \"tuning\",\n",
    "    \"naive-python-validation-noniterative-monotone\": \"validation\",\n",
    "    \"naive-java-meta-noniterative-monotone\": \"meta\",\n",
    "    \"naive-java-tuning-noniterative-monotone\": \"tuning\",\n",
    "    \"naive-java-validation-noniterative-monotone\": \"validation\"\n",
    "}\n",
    "\n",
    "dfSynergyPython = getSynnergyTable(dfResults[filter_python], col_labels = col_labels)[[\"filtering\", \"meta\", \"tuning\", \"validation\"]]\n",
    "dfSynergyJava = getSynnergyTable(dfResults[filter_java], col_labels = col_labels)[[\"meta\", \"tuning\", \"validation\"]]\n",
    "tournament_latex = pd.concat([dfSynergyJava, dfSynergyPython], axis=1).to_latex()\n",
    "print(tournament_latex.replace(\"{lrrrrrrr}\\n\\\\toprule\", \"{l||rrr|rrrr}\\n\\\\toprule\\n & \\multicolumn{3}{c|}{WEKA backend} & \\multicolumn{3}{c}{scikit-learn backend}\\\\\\\\\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1485, 'madelon', 2600, 501, 2]\n",
      "[1590, 'adult', 48842, 15, 2]\n",
      "[1515, 'micro-mass', 571, 1301, 20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1457, 'amazon-commerce-reviews', 1500, 10001, 50]\n",
      "[1475, 'first-order-theorem-proving', 6118, 52, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1468: cnae-9 to file /home/felix/.openml/cache/org/openml/www/datasets/1468/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1468, 'cnae-9', 1080, 857, 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1486: nomao to file /home/felix/.openml/cache/org/openml/www/datasets/1486/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1486, 'nomao', 34465, 119, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1489: phoneme to file /home/felix/.openml/cache/org/openml/www/datasets/1489/dataset.pkl.py3\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1489, 'phoneme', 5404, 6, 2]\n",
      "[23512, 'higgs', 98050, 29, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 23517: numerai28.6 to file /home/felix/.openml/cache/org/openml/www/datasets/23517/dataset.pkl.py3\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23517, 'numerai28.6', 96320, 22, 2]\n",
      "[4541, 'Diabetes130US', 101766, 50, 3]\n",
      "[4534, 'PhishingWebsites', 11055, 31, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 4538: GesturePhaseSegmentationProcessed to file /home/felix/.openml/cache/org/openml/www/datasets/4538/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4538, 'GesturePhaseSegmentationProcessed', 9873, 33, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4134, 'Bioresponse', 3751, 1777, 2]\n",
      "[4135, 'Amazon_employee_access', 32769, 10, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40978: Internet-Advertisements to file /home/felix/.openml/cache/org/openml/www/datasets/40978/dataset.pkl.py3\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40978, 'Internet-Advertisements', 3279, 1559, 2]\n",
      "[41027, 'jungle_chess_2pcs_raw_endgame_complete', 44819, 7, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40981: Australian to file /home/felix/.openml/cache/org/openml/www/datasets/40981/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40981, 'Australian', 690, 15, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40982: steel-plates-fault to file /home/felix/.openml/cache/org/openml/www/datasets/40982/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40982, 'steel-plates-fault', 1941, 28, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40983: wilt to file /home/felix/.openml/cache/org/openml/www/datasets/40983/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40983, 'wilt', 4839, 6, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40984: segment to file /home/felix/.openml/cache/org/openml/www/datasets/40984/dataset.pkl.py3\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40984, 'segment', 2310, 20, 7]\n",
      "[40701, 'churn', 5000, 21, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40670: dna to file /home/felix/.openml/cache/org/openml/www/datasets/40670/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40670, 'dna', 3186, 181, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40685: shuttle to file /home/felix/.openml/cache/org/openml/www/datasets/40685/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40685, 'shuttle', 58000, 10, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40900: Satellite to file /home/felix/.openml/cache/org/openml/www/datasets/40900/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40900, 'Satellite', 5100, 37, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1111: KDDCup09_appetency to file /home/felix/.openml/cache/org/openml/www/datasets/1111/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1111, 'KDDCup09_appetency', 50000, 231, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 42732: sf-police-incidents to file /home/felix/.openml/cache/org/openml/www/datasets/42732/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42732, 'sf-police-incidents', 2215023, 10, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/openml/_api_calls.py:105: UserWarning: Received uncompressed content from OpenML for https://www.openml.org/data/v1/download/22044769/Click_prediction_small.arff.\n",
      "  .format(url))\n",
      "DEBUG:openml.datasets.dataset:Saved dataset 42733: Click_prediction_small to file /home/felix/.openml/cache/org/openml/www/datasets/42733/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42733, 'Click_prediction_small', 39948, 12, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40498: wine-quality-white to file /home/felix/.openml/cache/org/openml/www/datasets/40498/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40498, 'wine-quality-white', 4898, 12, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41162: kick to file /home/felix/.openml/cache/org/openml/www/datasets/41162/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41162, 'kick', 72983, 33, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41163: dilbert to file /home/felix/.openml/cache/org/openml/www/datasets/41163/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41163, 'dilbert', 10000, 2001, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41164: fabert to file /home/felix/.openml/cache/org/openml/www/datasets/41164/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41164, 'fabert', 8237, 801, 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41165: robert to file /home/felix/.openml/cache/org/openml/www/datasets/41165/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41165, 'robert', 10000, 7201, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41166: volkert to file /home/felix/.openml/cache/org/openml/www/datasets/41166/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41166, 'volkert', 58310, 181, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41167: dionis to file /home/felix/.openml/cache/org/openml/www/datasets/41167/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41167, 'dionis', 416188, 61, 355]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41168: jannis to file /home/felix/.openml/cache/org/openml/www/datasets/41168/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41168, 'jannis', 83733, 55, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41169: helena to file /home/felix/.openml/cache/org/openml/www/datasets/41169/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41169, 'helena', 65196, 28, 100]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41142: christine to file /home/felix/.openml/cache/org/openml/www/datasets/41142/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41142, 'christine', 5418, 1637, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41143: jasmine to file /home/felix/.openml/cache/org/openml/www/datasets/41143/dataset.pkl.py3\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41143, 'jasmine', 2984, 145, 2]\n",
      "[41144, 'madeline', 3140, 260, 2]\n",
      "[41145, 'philippine', 5832, 309, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41146: sylvine to file /home/felix/.openml/cache/org/openml/www/datasets/41146/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41146, 'sylvine', 5124, 21, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41147, 'albert', 425240, 79, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41150: MiniBooNE to file /home/felix/.openml/cache/org/openml/www/datasets/41150/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41150, 'MiniBooNE', 130064, 51, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41156: ada to file /home/felix/.openml/cache/org/openml/www/datasets/41156/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41156, 'ada', 4147, 49, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41157: arcene to file /home/felix/.openml/cache/org/openml/www/datasets/41157/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41157, 'arcene', 100, 10001, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41158: gina to file /home/felix/.openml/cache/org/openml/www/datasets/41158/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41158, 'gina', 3153, 971, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41159: guillermo to file /home/felix/.openml/cache/org/openml/www/datasets/41159/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41159, 'guillermo', 20000, 4297, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 41138: APSFailure to file /home/felix/.openml/cache/org/openml/www/datasets/41138/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41138, 'APSFailure', 76000, 171, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 54: vehicle to file /home/felix/.openml/cache/org/openml/www/datasets/54/dataset.pkl.py3\n",
      "DEBUG:openml.datasets.dataset:Saved dataset 181: yeast to file /home/felix/.openml/cache/org/openml/www/datasets/181/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54, 'vehicle', 846, 19, 4]\n",
      "[181, 'yeast', 1484, 9, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1461: bank-marketing to file /home/felix/.openml/cache/org/openml/www/datasets/1461/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1461, 'bank-marketing', 45211, 17, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1494: qsar-biodeg to file /home/felix/.openml/cache/org/openml/www/datasets/1494/dataset.pkl.py3\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n",
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1494, 'qsar-biodeg', 1055, 42, 2]\n",
      "[1464, 'blood-transfusion-service-center', 748, 5, 2]\n",
      "[12, 'mfeat-factors', 2000, 217, 10]\n",
      "[3, 'kr-vs-kp', 3196, 37, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1487: ozone-level-8hr to file /home/felix/.openml/cache/org/openml/www/datasets/1487/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1487, 'ozone-level-8hr', 2534, 73, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40668: connect-4 to file /home/felix/.openml/cache/org/openml/www/datasets/40668/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40668, 'connect-4', 67557, 43, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1067: kc1 to file /home/felix/.openml/cache/org/openml/www/datasets/1067/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1067, 'kc1', 2109, 22, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 1049: pc4 to file /home/felix/.openml/cache/org/openml/www/datasets/1049/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1049, 'pc4', 1458, 38, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Saved dataset 40975: car to file /home/felix/.openml/cache/org/openml/www/datasets/40975/dataset.pkl.py3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40975, 'car', 1728, 7, 4]\n"
     ]
    }
   ],
   "source": [
    "datasets = [int(i) for i in pd.unique(dfResults[\"openmlid\"])]\n",
    "rows = []\n",
    "for openmlid in datasets:\n",
    "    dataset = openml.datasets.get_dataset(openmlid)\n",
    "    row = [openmlid, dataset.name, int(dataset.qualities[\"NumberOfInstances\"]), len(dataset.features), int(dataset.qualities[\"NumberOfClasses\"])]\n",
    "    rows.append(row)\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rlrrr}\n",
      "\\toprule\n",
      " openmlid &                                    name &  instances &  features &  classes \\\\\n",
      "\\midrule\n",
      "        3 &                                kr-vs-kp &       3196 &        37 &        2 \\\\\n",
      "       12 &                           mfeat-factors &       2000 &       217 &       10 \\\\\n",
      "       54 &                                 vehicle &        846 &        19 &        4 \\\\\n",
      "      181 &                                   yeast &       1484 &         9 &       10 \\\\\n",
      "     1049 &                                     pc4 &       1458 &        38 &        2 \\\\\n",
      "     1067 &                                     kc1 &       2109 &        22 &        2 \\\\\n",
      "     1111 &                      KDDCup09\\_appetency &      50000 &       231 &        2 \\\\\n",
      "     1457 &                 amazon-commerce-reviews &       1500 &     10001 &       50 \\\\\n",
      "     1461 &                          bank-marketing &      45211 &        17 &        2 \\\\\n",
      "     1464 &        blood-transfusion-service-center &        748 &         5 &        2 \\\\\n",
      "     1468 &                                  cnae-9 &       1080 &       857 &        9 \\\\\n",
      "     1475 &             first-order-theorem-proving &       6118 &        52 &        6 \\\\\n",
      "     1485 &                                 madelon &       2600 &       501 &        2 \\\\\n",
      "     1486 &                                   nomao &      34465 &       119 &        2 \\\\\n",
      "     1487 &                         ozone-level-8hr &       2534 &        73 &        2 \\\\\n",
      "     1489 &                                 phoneme &       5404 &         6 &        2 \\\\\n",
      "     1494 &                             qsar-biodeg &       1055 &        42 &        2 \\\\\n",
      "     1515 &                              micro-mass &        571 &      1301 &       20 \\\\\n",
      "     1590 &                                   adult &      48842 &        15 &        2 \\\\\n",
      "     4134 &                             Bioresponse &       3751 &      1777 &        2 \\\\\n",
      "     4135 &                  Amazon\\_employee\\_access &      32769 &        10 &        2 \\\\\n",
      "     4534 &                        PhishingWebsites &      11055 &        31 &        2 \\\\\n",
      "     4538 &       GesturePhaseSegmentationProcessed &       9873 &        33 &        5 \\\\\n",
      "     4541 &                           Diabetes130US &     101766 &        50 &        3 \\\\\n",
      "    23512 &                                   higgs &      98050 &        29 &        2 \\\\\n",
      "    23517 &                             numerai28.6 &      96320 &        22 &        2 \\\\\n",
      "    40498 &                      wine-quality-white &       4898 &        12 &        7 \\\\\n",
      "    40668 &                               connect-4 &      67557 &        43 &        3 \\\\\n",
      "    40670 &                                     dna &       3186 &       181 &        3 \\\\\n",
      "    40685 &                                 shuttle &      58000 &        10 &        7 \\\\\n",
      "    40701 &                                   churn &       5000 &        21 &        2 \\\\\n",
      "    40900 &                               Satellite &       5100 &        37 &        2 \\\\\n",
      "    40975 &                                     car &       1728 &         7 &        4 \\\\\n",
      "    40978 &                 Internet-Advertisements &       3279 &      1559 &        2 \\\\\n",
      "    40981 &                              Australian &        690 &        15 &        2 \\\\\n",
      "    40982 &                      steel-plates-fault &       1941 &        28 &        7 \\\\\n",
      "    40983 &                                    wilt &       4839 &         6 &        2 \\\\\n",
      "    40984 &                                 segment &       2310 &        20 &        7 \\\\\n",
      "    41027 &  jungle\\_chess\\_2pcs\\_raw\\_endgame\\_complete &      44819 &         7 &        3 \\\\\n",
      "    41138 &                              APSFailure &      76000 &       171 &        2 \\\\\n",
      "    41142 &                               christine &       5418 &      1637 &        2 \\\\\n",
      "    41143 &                                 jasmine &       2984 &       145 &        2 \\\\\n",
      "    41144 &                                madeline &       3140 &       260 &        2 \\\\\n",
      "    41145 &                              philippine &       5832 &       309 &        2 \\\\\n",
      "    41146 &                                 sylvine &       5124 &        21 &        2 \\\\\n",
      "    41147 &                                  albert &     425240 &        79 &        2 \\\\\n",
      "    41150 &                               MiniBooNE &     130064 &        51 &        2 \\\\\n",
      "    41156 &                                     ada &       4147 &        49 &        2 \\\\\n",
      "    41157 &                                  arcene &        100 &     10001 &        2 \\\\\n",
      "    41158 &                                    gina &       3153 &       971 &        2 \\\\\n",
      "    41159 &                               guillermo &      20000 &      4297 &        2 \\\\\n",
      "    41162 &                                    kick &      72983 &        33 &        2 \\\\\n",
      "    41163 &                                 dilbert &      10000 &      2001 &        5 \\\\\n",
      "    41164 &                                  fabert &       8237 &       801 &        7 \\\\\n",
      "    41165 &                                  robert &      10000 &      7201 &       10 \\\\\n",
      "    41166 &                                 volkert &      58310 &       181 &       10 \\\\\n",
      "    41167 &                                  dionis &     416188 &        61 &      355 \\\\\n",
      "    41168 &                                  jannis &      83733 &        55 &        4 \\\\\n",
      "    41169 &                                  helena &      65196 &        28 &      100 \\\\\n",
      "    42732 &                     sf-police-incidents &    2215023 &        10 &        2 \\\\\n",
      "    42733 &                  Click\\_prediction\\_small &      39948 &        12 &        2 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDatasets = pd.DataFrame(rows, columns=[\"openmlid\", \"name\", \"instances\", \"features\", \"classes\"]).sort_values(\"openmlid\")\n",
    "print(dfDatasets.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2958: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2972: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrrrr}\n",
      "\\toprule\n",
      " openmlid &                                                                                                                               asklearn &                  primitive &                    scaling &                  filtering &                       meta &                     tuning &                 validation \\\\\n",
      "\\midrule\n",
      "        3 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &                        nan &       \\textbf{0.0$\\pm$0.0} &      \\textbf{0.0$\\pm$0.01} \\\\\n",
      "       12 &  \\underline{0.03$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.02$\\pm$0.02} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.02} &     \\textbf{0.02$\\pm$0.02} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.02} \\\\\n",
      "       54 &                                                                0.19$\\pm$0.03 $\\circ$ $\\circ$ $\\circ$ \\phantom{$\\circ$} $\\circ$ $\\circ$ &     \\textbf{0.16$\\pm$0.03} &     \\textbf{0.16$\\pm$0.03} &     \\textbf{0.16$\\pm$0.03} &     \\textbf{0.16$\\pm$0.03} &              0.18$\\pm$0.03 &     \\textbf{0.16$\\pm$0.03} \\\\\n",
      "      181 &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.38$\\pm$0.03} &  \\underline{0.39$\\pm$0.05} &  \\underline{0.39$\\pm$0.04} &     \\textbf{0.38$\\pm$0.03} &  \\underline{0.39$\\pm$0.04} &  \\underline{0.39$\\pm$0.04} \\\\\n",
      "     1049 &  \\underline{0.09$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.09$\\pm$0.02} &  \\underline{0.09$\\pm$0.02} &     \\textbf{0.08$\\pm$0.01} &  \\underline{0.09$\\pm$0.01} &  \\underline{0.09$\\pm$0.01} &     \\textbf{0.08$\\pm$0.01} \\\\\n",
      "     1067 &     \\textbf{0.14$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.15$\\pm$0.02} &     \\textbf{0.14$\\pm$0.01} &     \\textbf{0.14$\\pm$0.02} &     \\textbf{0.14$\\pm$0.02} &  \\underline{0.15$\\pm$0.01} &  \\underline{0.15$\\pm$0.01} \\\\\n",
      "     1111 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &                        nan &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} \\\\\n",
      "     1457 &                                                         0.2$\\pm$0.02 $\\bullet$ $\\circ$ $\\bullet$ \\phantom{$\\circ$} $\\bullet$ $\\bullet$ &              0.23$\\pm$0.04 &     \\textbf{0.15$\\pm$0.03} &              0.23$\\pm$0.04 &              0.22$\\pm$0.02 &              0.21$\\pm$0.06 &              0.23$\\pm$0.04 \\\\\n",
      "     1461 &      \\textbf{0.11$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.11$\\pm$0.0} &      \\textbf{0.11$\\pm$0.0} &      \\textbf{0.11$\\pm$0.0} &                        nan &      \\textbf{0.11$\\pm$0.0} &      \\textbf{0.11$\\pm$0.0} \\\\\n",
      "     1464 &  \\underline{0.22$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.22$\\pm$0.02} &     \\textbf{0.21$\\pm$0.02} &              0.23$\\pm$0.01 &  \\underline{0.22$\\pm$0.02} &  \\underline{0.22$\\pm$0.03} &              0.23$\\pm$0.02 \\\\\n",
      "     1468 &  \\underline{0.05$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.04$\\pm$0.02} &     \\textbf{0.04$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &     \\textbf{0.04$\\pm$0.02} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} \\\\\n",
      "     1475 &                                                                          0.39$\\pm$0.03 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} \\\\\n",
      "     1485 &                                             \\textbf{0.11$\\pm$0.02} $\\dagger$ $\\dagger$ \\phantom{$\\circ$} $\\dagger$ $\\dagger$ $\\dagger$ &              0.26$\\pm$0.03 &              0.26$\\pm$0.03 &  \\underline{0.12$\\pm$0.02} &              0.21$\\pm$0.03 &              0.25$\\pm$0.03 &              0.27$\\pm$0.03 \\\\\n",
      "     1486 &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &                        nan &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} \\\\\n",
      "     1487 &     \\textbf{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &  \\underline{0.06$\\pm$0.01} &  \\underline{0.06$\\pm$0.01} \\\\\n",
      "     1489 &   \\underline{0.1$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} \\\\\n",
      "     1494 &                                  0.14$\\pm$0.03 \\phantom{$\\circ$} $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.03} &     \\textbf{0.12$\\pm$0.03} &     \\textbf{0.12$\\pm$0.03} &     \\textbf{0.12$\\pm$0.03} &     \\textbf{0.12$\\pm$0.03} &  \\underline{0.13$\\pm$0.03} \\\\\n",
      "     1515 &                                          \\underline{0.07$\\pm$0.03} $\\bullet$ \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ &              0.13$\\pm$0.03 &     \\textbf{0.06$\\pm$0.02} &              0.13$\\pm$0.03 &              0.13$\\pm$0.03 &              0.14$\\pm$0.04 &              0.13$\\pm$0.03 \\\\\n",
      "     1590 &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.16$\\pm$0.0} &      \\textbf{0.16$\\pm$0.0} &      \\textbf{0.16$\\pm$0.0} &                        nan &      \\textbf{0.16$\\pm$0.0} &      \\textbf{0.16$\\pm$0.0} \\\\\n",
      "     4134 &   \\underline{0.2$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.19$\\pm$0.02} &     \\textbf{0.19$\\pm$0.02} &     \\textbf{0.19$\\pm$0.02} &     \\textbf{0.19$\\pm$0.02} &   \\underline{0.2$\\pm$0.02} &   \\underline{0.2$\\pm$0.02} \\\\\n",
      "     4135 &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &                        nan &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} \\\\\n",
      "     4534 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &      \\textbf{0.02$\\pm$0.0} &                        nan &      \\textbf{0.02$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "     4538 &                        0.31$\\pm$0.01 \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.3$\\pm$0.01} &   \\underline{0.3$\\pm$0.01} &   \\underline{0.3$\\pm$0.01} &   \\underline{0.3$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &   \\underline{0.3$\\pm$0.01} \\\\\n",
      "     4541 &      \\textbf{0.42$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.42$\\pm$0.0} &      \\textbf{0.42$\\pm$0.0} &      \\textbf{0.42$\\pm$0.0} &                        nan &      \\textbf{0.42$\\pm$0.0} &      \\textbf{0.42$\\pm$0.0} \\\\\n",
      "    23512 &      \\textbf{0.28$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.28$\\pm$0.01} &     \\textbf{0.28$\\pm$0.01} &     \\textbf{0.28$\\pm$0.01} &     \\textbf{0.28$\\pm$0.01} &      \\textbf{0.28$\\pm$0.0} &     \\textbf{0.28$\\pm$0.01} \\\\\n",
      "    23517 &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.48$\\pm$0.0} &      \\textbf{0.48$\\pm$0.0} &     \\textbf{0.48$\\pm$0.01} &     \\textbf{0.48$\\pm$0.01} &      \\textbf{0.48$\\pm$0.0} &      \\textbf{0.48$\\pm$0.0} \\\\\n",
      "    40498 &                                                                          0.34$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &     \\textbf{0.29$\\pm$0.01} &   \\underline{0.3$\\pm$0.02} &     \\textbf{0.29$\\pm$0.02} &     \\textbf{0.29$\\pm$0.02} &     \\textbf{0.29$\\pm$0.01} &   \\underline{0.3$\\pm$0.02} \\\\\n",
      "    40668 &                                             \\textbf{0.17$\\pm$0.06} $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ \\phantom{$\\circ$} &               0.26$\\pm$0.0 &               0.26$\\pm$0.0 &              0.26$\\pm$0.06 &                        nan &              0.26$\\pm$0.01 &              0.26$\\pm$0.07 \\\\\n",
      "    40670 &     \\textbf{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &                        nan &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} \\\\\n",
      "    40685 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "    40701 &      \\textbf{0.04$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &                        nan &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} \\\\\n",
      "    40900 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    40975 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.01$\\pm$0.01} &  \\underline{0.01$\\pm$0.01} &  \\underline{0.01$\\pm$0.01} &                        nan &       \\textbf{0.0$\\pm$0.0} &  \\underline{0.01$\\pm$0.01} \\\\\n",
      "    40978 &     \\textbf{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &                        nan &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} \\\\\n",
      "    40981 &  \\underline{0.12$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.12$\\pm$0.03} &              0.13$\\pm$0.04 &              0.13$\\pm$0.03 &                        nan &     \\textbf{0.11$\\pm$0.03} &              0.13$\\pm$0.03 \\\\\n",
      "    40982 &  \\underline{0.21$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.21$\\pm$0.02} &  \\underline{0.22$\\pm$0.02} &  \\underline{0.21$\\pm$0.02} &  \\underline{0.21$\\pm$0.03} &  \\underline{0.21$\\pm$0.03} &      \\textbf{0.2$\\pm$0.03} \\\\\n",
      "    40983 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &  \\underline{0.02$\\pm$0.01} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} \\\\\n",
      "    40984 &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.02$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "    41027 &                                                     \\textbf{0.07$\\pm$0.01} $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ &              0.16$\\pm$0.01 &               0.14$\\pm$0.0 &              0.16$\\pm$0.01 &              0.16$\\pm$0.01 &              0.15$\\pm$0.01 &              0.16$\\pm$0.01 \\\\\n",
      "    41138 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    41142 &     \\textbf{0.27$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.28$\\pm$0.02} &     \\textbf{0.27$\\pm$0.02} &     \\textbf{0.27$\\pm$0.02} &                        nan &  \\underline{0.28$\\pm$0.02} &     \\textbf{0.27$\\pm$0.02} \\\\\n",
      "    41143 &     \\textbf{0.18$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.19$\\pm$0.02} &  \\underline{0.19$\\pm$0.03} &     \\textbf{0.18$\\pm$0.02} &                        nan &  \\underline{0.19$\\pm$0.03} &     \\textbf{0.18$\\pm$0.01} \\\\\n",
      "    41144 &                                              \\textbf{0.1$\\pm$0.02} $\\dagger$ $\\dagger$ \\phantom{$\\circ$} $\\dagger$ $\\dagger$ $\\dagger$ &              0.24$\\pm$0.02 &              0.23$\\pm$0.01 &      \\textbf{0.1$\\pm$0.03} &               0.2$\\pm$0.01 &              0.21$\\pm$0.02 &              0.24$\\pm$0.02 \\\\\n",
      "    41145 &                                                     \\textbf{0.18$\\pm$0.01} $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ &              0.25$\\pm$0.02 &              0.25$\\pm$0.01 &              0.24$\\pm$0.01 &              0.24$\\pm$0.01 &              0.25$\\pm$0.02 &              0.24$\\pm$0.01 \\\\\n",
      "    41146 &     \\textbf{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} \\\\\n",
      "    41147 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.32$\\pm$0.0} &      \\textbf{0.32$\\pm$0.0} &                        nan &                        nan &                        nan &      \\textbf{0.32$\\pm$0.0} \\\\\n",
      "    41150 &      \\textbf{0.06$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.07$\\pm$0.0} &      \\textbf{0.06$\\pm$0.0} &   \\underline{0.07$\\pm$0.0} &   \\underline{0.07$\\pm$0.0} &   \\underline{0.07$\\pm$0.0} &   \\underline{0.07$\\pm$0.0} \\\\\n",
      "    41156 &  \\underline{0.14$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.14$\\pm$0.01} &  \\underline{0.14$\\pm$0.01} &  \\underline{0.14$\\pm$0.01} &     \\textbf{0.13$\\pm$0.01} &  \\underline{0.14$\\pm$0.01} &  \\underline{0.14$\\pm$0.01} \\\\\n",
      "    41157 &                                                                0.24$\\pm$0.09 $\\circ$ $\\circ$ \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ &     \\textbf{0.11$\\pm$0.09} &  \\underline{0.15$\\pm$0.08} &              0.22$\\pm$0.12 &  \\underline{0.12$\\pm$0.09} &  \\underline{0.14$\\pm$0.11} &     \\textbf{0.11$\\pm$0.07} \\\\\n",
      "    41158 &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} \\\\\n",
      "    41159 &             \\textbf{0.17$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\dagger$ \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.18$\\pm$0.01} &  \\underline{0.18$\\pm$0.01} &     \\textbf{0.17$\\pm$0.01} &  \\underline{0.18$\\pm$0.01} &              0.19$\\pm$0.01 &  \\underline{0.18$\\pm$0.01} \\\\\n",
      "    41162 &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &                        nan &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} \\\\\n",
      "    41163 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} \\\\\n",
      "    41164 &                                                                0.31$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ \\phantom{$\\circ$} $\\circ$ &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &   \\underline{0.3$\\pm$0.01} \\\\\n",
      "    41165 &     \\textbf{0.55$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.55$\\pm$0.01} &  \\underline{0.56$\\pm$0.01} &     \\textbf{0.55$\\pm$0.01} &  \\underline{0.56$\\pm$0.01} &     \\textbf{0.55$\\pm$0.01} &  \\underline{0.56$\\pm$0.02} \\\\\n",
      "    41166 &                                                      \\textbf{0.28$\\pm$0.0} $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ &               0.33$\\pm$0.0 &               0.32$\\pm$0.0 &               0.33$\\pm$0.0 &              0.33$\\pm$0.01 &              0.33$\\pm$0.01 &               0.33$\\pm$0.0 \\\\\n",
      "    41167 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.31$\\pm$0.0} &                        nan &                        nan &                        nan &     \\textbf{0.24$\\pm$0.01} &                        nan \\\\\n",
      "    41168 &                                                      \\textbf{0.27$\\pm$0.0} $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ &              0.29$\\pm$0.01 &               0.29$\\pm$0.0 &              0.29$\\pm$0.01 &               0.29$\\pm$0.0 &               0.29$\\pm$0.0 &               0.29$\\pm$0.0 \\\\\n",
      "    41169 &                                                                          0.69$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &   \\underline{0.65$\\pm$0.0} &      \\textbf{0.64$\\pm$0.0} &   \\underline{0.65$\\pm$0.0} &   \\underline{0.65$\\pm$0.0} &   \\underline{0.65$\\pm$0.0} &      \\textbf{0.64$\\pm$0.0} \\\\\n",
      "    42732 &      \\textbf{0.12$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &                        nan &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} \\\\\n",
      "    42733 &      \\textbf{0.17$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.17$\\pm$0.0} &      \\textbf{0.17$\\pm$0.0} &      \\textbf{0.17$\\pm$0.0} &                        nan &      \\textbf{0.17$\\pm$0.0} &      \\textbf{0.17$\\pm$0.0} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_python = ((dfResults[\"algorithm\"].str.contains(\"python\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")))\n",
    "filter_python = filter_python & ~dfResults[\"algorithm\"].str.contains(\"wrapping\")\n",
    "filter_python = filter_python | (dfResults[\"algorithm\"] == \"auto-sklearn\")\n",
    "filter_monotone = (~dfResults[\"algorithm\"].str.contains(\"monotone\"))\n",
    "filter_results = (dfResults[\"algorithm\"] == \"auto-sklearn\")\n",
    "filter_results = filter_results | (filter_python & filter_monotone)\n",
    "filter_results = filter_results | (filter_python & (dfResults[\"algorithm\"].str.contains(\"primitive\")))\n",
    "filter_results = filter_results | (filter_python & (dfResults[\"algorithm\"].str.contains(\"scaling\")))\n",
    "\n",
    "dfResultsMonotone = getResultTable(dfResults[filter_results], \"openmlid\", \"algorithm\", \"errorrate\", col_labels=col_labels_python, row_formatter=row_layouter)[[\"openmlid\", \"asklearn\", \"primitive\", \"scaling\", \"filtering\", \"meta\", \"tuning\", \"validation\"]]\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    print(dfResultsMonotone.to_latex(index=False, escape=False).replace(\"{rlllllll}\", \"{rrrrrrrr}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2958: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2972: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrrrr}\n",
      "\\toprule\n",
      " openmlid &                                                                                                            auto-weka &                                                                                                               mlplan &                  primitive &                  filtering &                       meta &                     tuning &                 validation \\\\\n",
      "\\midrule\n",
      "        3 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.01$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "       12 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.04$\\pm$0.01 \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} \\\\\n",
      "       54 &  \\underline{0.18$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.18$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.17$\\pm$0.02} &     \\textbf{0.17$\\pm$0.02} &  \\underline{0.18$\\pm$0.02} &     \\textbf{0.17$\\pm$0.02} &  \\underline{0.18$\\pm$0.04} \\\\\n",
      "      181 &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.38$\\pm$0.03} &     \\textbf{0.38$\\pm$0.02} &     \\textbf{0.38$\\pm$0.03} &     \\textbf{0.38$\\pm$0.02} &     \\textbf{0.38$\\pm$0.03} \\\\\n",
      "     1049 &   \\underline{0.1$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.02} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.02} \\\\\n",
      "     1067 &  \\underline{0.15$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.15$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.15$\\pm$0.02} &     \\textbf{0.14$\\pm$0.02} &  \\underline{0.15$\\pm$0.02} &  \\underline{0.15$\\pm$0.02} &  \\underline{0.15$\\pm$0.01} \\\\\n",
      "     1111 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "     1457 &   \\underline{0.29$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.25$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.25$\\pm$0.03} &     \\textbf{0.25$\\pm$0.04} &     \\textbf{0.25$\\pm$0.03} &     \\textbf{0.25$\\pm$0.03} &     \\textbf{0.25$\\pm$0.04} \\\\\n",
      "     1461 &    \\underline{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.1$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &    \\underline{0.1$\\pm$0.0} &    \\underline{0.1$\\pm$0.0} &    \\underline{0.1$\\pm$0.0} &      \\textbf{0.09$\\pm$0.0} &    \\underline{0.1$\\pm$0.0} \\\\\n",
      "     1464 &  \\underline{0.22$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                  0.24$\\pm$0.01 $\\circ$ \\phantom{$\\circ$} $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.21$\\pm$0.04} &  \\underline{0.22$\\pm$0.03} &     \\textbf{0.21$\\pm$0.04} &  \\underline{0.22$\\pm$0.04} &  \\underline{0.22$\\pm$0.03} \\\\\n",
      "     1468 &  \\underline{0.06$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.02} \\\\\n",
      "     1475 &              0.38$\\pm$0.03 \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.36$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.37$\\pm$0.02} &  \\underline{0.37$\\pm$0.02} &  \\underline{0.37$\\pm$0.02} &  \\underline{0.37$\\pm$0.02} &  \\underline{0.37$\\pm$0.02} \\\\\n",
      "     1485 &                                                                 0.39$\\pm$0.1 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                                  0.23$\\pm$0.05 \\phantom{$\\circ$} $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.23$\\pm$0.02 &     \\textbf{0.11$\\pm$0.02} &               0.2$\\pm$0.03 &              0.24$\\pm$0.04 &              0.24$\\pm$0.03 \\\\\n",
      "     1486 &      \\textbf{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                                0.05$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &      \\textbf{0.03$\\pm$0.0} &      \\textbf{0.03$\\pm$0.0} &      \\textbf{0.03$\\pm$0.0} &      \\textbf{0.03$\\pm$0.0} &      \\textbf{0.03$\\pm$0.0} \\\\\n",
      "     1487 &     \\textbf{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.06$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} \\\\\n",
      "     1489 &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} \\\\\n",
      "     1494 &  \\underline{0.13$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.02} &  \\underline{0.13$\\pm$0.03} &  \\underline{0.13$\\pm$0.03} &     \\textbf{0.12$\\pm$0.04} &  \\underline{0.13$\\pm$0.03} \\\\\n",
      "     1515 &     \\textbf{0.09$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.04} &  \\underline{0.13$\\pm$0.04} &  \\underline{0.14$\\pm$0.04} &  \\underline{0.13$\\pm$0.04} &  \\underline{0.14$\\pm$0.04} \\\\\n",
      "     1590 &      \\textbf{0.14$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.14$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.14$\\pm$0.0} &      \\textbf{0.14$\\pm$0.0} &      \\textbf{0.14$\\pm$0.0} &      \\textbf{0.14$\\pm$0.0} &      \\textbf{0.14$\\pm$0.0} \\\\\n",
      "     4134 &  \\underline{0.23$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.2$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.2$\\pm$0.01} &      \\textbf{0.2$\\pm$0.02} &      \\textbf{0.2$\\pm$0.01} &      \\textbf{0.2$\\pm$0.01} &      \\textbf{0.2$\\pm$0.01} \\\\\n",
      "     4135 &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.06$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} \\\\\n",
      "     4534 &   \\underline{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                                0.04$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} \\\\\n",
      "     4538 &  \\underline{0.32$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.32$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.32$\\pm$0.01} &  \\underline{0.32$\\pm$0.01} &     \\textbf{0.31$\\pm$0.01} &  \\underline{0.32$\\pm$0.01} &  \\underline{0.32$\\pm$0.01} \\\\\n",
      "     4541 &   \\underline{0.43$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.42$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.42$\\pm$0.01} &     \\textbf{0.42$\\pm$0.01} &     \\textbf{0.42$\\pm$0.01} &     \\textbf{0.42$\\pm$0.01} &     \\textbf{0.42$\\pm$0.01} \\\\\n",
      "    23512 &                         0.31$\\pm$0.0 \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ &                        0.32$\\pm$0.02 \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ &              0.31$\\pm$0.02 &              0.31$\\pm$0.01 &              0.31$\\pm$0.02 &              0.31$\\pm$0.01 &     \\textbf{0.28$\\pm$0.01} \\\\\n",
      "    23517 &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.48$\\pm$0.0} &   \\underline{0.49$\\pm$0.0} &      \\textbf{0.48$\\pm$0.0} &      \\textbf{0.48$\\pm$0.0} &     \\textbf{0.48$\\pm$0.01} \\\\\n",
      "    40498 &  \\underline{0.31$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.31$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} \\\\\n",
      "    40668 &                      \\textbf{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ \\phantom{$\\circ$} $\\bullet$ &              0.19$\\pm$0.01 \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.21$\\pm$0.11} &  \\underline{0.18$\\pm$0.12} &              0.26$\\pm$0.08 &  \\underline{0.21$\\pm$0.11} &               0.24$\\pm$0.1 \\\\\n",
      "    40670 &  \\underline{0.04$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.05$\\pm$0.01 \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.04$\\pm$0.01} &  \\underline{0.04$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &  \\underline{0.04$\\pm$0.01} &  \\underline{0.04$\\pm$0.01} \\\\\n",
      "    40685 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "    40701 &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.04$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} \\\\\n",
      "    40900 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    40975 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.0$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} \\\\\n",
      "    40978 &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "    40981 &                                  0.14$\\pm$0.05 $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                  0.14$\\pm$0.03 $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.12$\\pm$0.03} &     \\textbf{0.12$\\pm$0.04} &  \\underline{0.13$\\pm$0.03} &              0.14$\\pm$0.02 &              0.14$\\pm$0.02 \\\\\n",
      "    40982 &     \\textbf{0.19$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.2$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.2$\\pm$0.03} &   \\underline{0.2$\\pm$0.03} &     \\textbf{0.19$\\pm$0.02} &   \\underline{0.2$\\pm$0.02} &   \\underline{0.2$\\pm$0.03} \\\\\n",
      "    40983 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} \\\\\n",
      "    40984 &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                                0.04$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "    41027 &                                                                0.16$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                                                                0.15$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &      \\textbf{0.09$\\pm$0.0} &      \\textbf{0.09$\\pm$0.0} &     \\textbf{0.09$\\pm$0.01} &      \\textbf{0.09$\\pm$0.0} &      \\textbf{0.09$\\pm$0.0} \\\\\n",
      "    41138 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &     \\textbf{0.01$\\pm$0.01} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    41142 &                                                                0.29$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                        0.28$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.27$\\pm$0.02 &     \\textbf{0.25$\\pm$0.02} &              0.27$\\pm$0.02 &              0.27$\\pm$0.02 &              0.27$\\pm$0.02 \\\\\n",
      "    41143 &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.17$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.17$\\pm$0.02} &  \\underline{0.17$\\pm$0.02} &  \\underline{0.17$\\pm$0.02} &  \\underline{0.17$\\pm$0.02} &  \\underline{0.17$\\pm$0.02} \\\\\n",
      "    41144 &  \\underline{0.27$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.19$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.21$\\pm$0.05 &     \\textbf{0.11$\\pm$0.01} &              0.18$\\pm$0.03 &               0.2$\\pm$0.03 &               0.2$\\pm$0.04 \\\\\n",
      "    41145 &                                                                0.29$\\pm$0.03 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                                  0.26$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.25$\\pm$0.01 &     \\textbf{0.13$\\pm$0.03} &              0.24$\\pm$0.01 &              0.25$\\pm$0.01 &              0.26$\\pm$0.02 \\\\\n",
      "    41146 &      \\textbf{0.04$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                \\underline{0.07$\\pm$0.01} $\\circ$ $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.06$\\pm$0.01} &  \\underline{0.06$\\pm$0.01} \\\\\n",
      "    41147 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                             \\textbf{0.32$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ &  \\underline{0.33$\\pm$0.01} &   \\underline{0.33$\\pm$0.0} &              0.41$\\pm$0.08 &              0.39$\\pm$0.07 &              0.36$\\pm$0.06 \\\\\n",
      "    41150 &      \\textbf{0.07$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.08$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.09$\\pm$0.0} &   \\underline{0.09$\\pm$0.0} &   \\underline{0.09$\\pm$0.0} &   \\underline{0.09$\\pm$0.0} &   \\underline{0.09$\\pm$0.0} \\\\\n",
      "    41156 &     \\textbf{0.15$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.15$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.15$\\pm$0.02} &     \\textbf{0.15$\\pm$0.01} \\\\\n",
      "    41157 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.21$\\pm$0.14 $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.15$\\pm$0.09} &               0.3$\\pm$0.14 &  \\underline{0.16$\\pm$0.21} &              0.18$\\pm$0.11 &              0.24$\\pm$0.14 \\\\\n",
      "    41158 &  \\underline{0.07$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\underline{0.07$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ \\phantom{$\\circ$} &  \\underline{0.07$\\pm$0.04} &     \\textbf{0.06$\\pm$0.03} &               0.1$\\pm$0.06 &               0.1$\\pm$0.06 &  \\underline{0.07$\\pm$0.04} \\\\\n",
      "    41159 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                 0.2$\\pm$0.01 \\phantom{$\\circ$} $\\circ$ \\phantom{$\\circ$} $\\bullet$ \\phantom{$\\circ$} &              0.21$\\pm$0.01 &     \\textbf{0.17$\\pm$0.01} &              0.21$\\pm$0.01 &              0.22$\\pm$0.01 &              0.21$\\pm$0.01 \\\\\n",
      "    41162 &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} \\\\\n",
      "    41163 &   \\underline{0.08$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.03$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} &   \\underline{0.03$\\pm$0.0} &   \\underline{0.03$\\pm$0.0} &   \\underline{0.03$\\pm$0.0} \\\\\n",
      "    41164 &                                                                0.35$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                        0.32$\\pm$0.02 $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.3$\\pm$0.01} &  \\underline{0.31$\\pm$0.01} &  \\underline{0.31$\\pm$0.01} &  \\underline{0.31$\\pm$0.01} &  \\underline{0.31$\\pm$0.01} \\\\\n",
      "    41165 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                            0.62$\\pm$0.02 \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ &               0.6$\\pm$0.03 &              0.61$\\pm$0.03 &  \\underline{0.59$\\pm$0.02} &               0.6$\\pm$0.02 &     \\textbf{0.58$\\pm$0.02} \\\\\n",
      "    41166 &  \\underline{0.36$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                                0.35$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} \\\\\n",
      "    41167 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                                 0.96$\\pm$0.0 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &  \\underline{0.12$\\pm$0.07} &      \\textbf{0.09$\\pm$0.0} &  \\underline{0.11$\\pm$0.05} &      \\textbf{0.09$\\pm$0.0} &              0.11$\\pm$0.05 \\\\\n",
      "    41168 &  \\underline{0.34$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                    \\underline{0.3$\\pm$0.0} $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ \\phantom{$\\circ$} &              0.34$\\pm$0.01 &              0.34$\\pm$0.02 &              0.34$\\pm$0.01 &              0.34$\\pm$0.01 &     \\textbf{0.29$\\pm$0.02} \\\\\n",
      "    41169 &      \\textbf{0.71$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.72$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.72$\\pm$0.0} &   \\underline{0.72$\\pm$0.0} &  \\underline{0.72$\\pm$0.01} &  \\underline{0.72$\\pm$0.01} &  \\underline{0.72$\\pm$0.01} \\\\\n",
      "    42732 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.12$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} \\\\\n",
      "    42733 &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.17$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.17$\\pm$0.0} &   \\underline{0.17$\\pm$0.0} &   \\underline{0.17$\\pm$0.0} &      \\textbf{0.16$\\pm$0.0} &   \\underline{0.17$\\pm$0.0} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_weka = ((dfResults[\"algorithm\"].str.contains(\"java\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")))\n",
    "filter_weka = filter_weka & ~dfResults[\"algorithm\"].str.contains(\"wrapping\")\n",
    "filter_monotone = ~(dfResults[\"algorithm\"].str.contains(\"monotone\"))\n",
    "filter_results = (dfResults[\"algorithm\"] == \"auto-weka\") | (dfResults[\"algorithm\"] == \"mlplan\")\n",
    "filter_results = filter_results | (filter_weka & filter_monotone)\n",
    "\n",
    "dfResultsMonotone = getResultTable(dfResults[filter_results], \"openmlid\", \"algorithm\", \"errorrate\", col_labels=col_labels_python, row_formatter=row_layouter)[[\"openmlid\", \"auto-weka\", \"mlplan\", \"primitive\", \"filtering\", \"meta\", \"tuning\", \"validation\"]]\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    print(dfResultsMonotone.to_latex(index=False, escape=False).replace(\"{rlllllll}\", \"{rrrrrrrr}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## monotone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2958: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2972: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrrrr}\n",
      "\\toprule\n",
      " openmlid &                                                                                                                               asklearn &                  primitive &                    scaling &                  filtering &                       meta &                     tuning &                       full \\\\\n",
      "\\midrule\n",
      "        3 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &                        nan &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "       12 &  \\underline{0.03$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.02$\\pm$0.02} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.02} &     \\textbf{0.02$\\pm$0.01} &  \\underline{0.03$\\pm$0.02} \\\\\n",
      "       54 &                                                                          0.19$\\pm$0.03 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &     \\textbf{0.16$\\pm$0.03} &     \\textbf{0.16$\\pm$0.03} &     \\textbf{0.16$\\pm$0.03} &     \\textbf{0.16$\\pm$0.03} &  \\underline{0.17$\\pm$0.03} &  \\underline{0.17$\\pm$0.03} \\\\\n",
      "      181 &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.38$\\pm$0.03} &              0.39$\\pm$0.05 &  \\underline{0.39$\\pm$0.04} &  \\underline{0.39$\\pm$0.02} &  \\underline{0.38$\\pm$0.03} &     \\textbf{0.37$\\pm$0.05} \\\\\n",
      "     1049 &  \\underline{0.09$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.09$\\pm$0.02} &  \\underline{0.09$\\pm$0.02} &     \\textbf{0.08$\\pm$0.02} &  \\underline{0.09$\\pm$0.01} &               0.1$\\pm$0.02 &  \\underline{0.09$\\pm$0.01} \\\\\n",
      "     1067 &     \\textbf{0.14$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.15$\\pm$0.02} &     \\textbf{0.14$\\pm$0.01} &  \\underline{0.15$\\pm$0.01} &     \\textbf{0.14$\\pm$0.01} &     \\textbf{0.14$\\pm$0.02} &  \\underline{0.15$\\pm$0.01} \\\\\n",
      "     1111 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &                        nan &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} \\\\\n",
      "     1457 &                                                                         0.2$\\pm$0.02 $\\bullet$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &              0.23$\\pm$0.04 &     \\textbf{0.15$\\pm$0.03} &     \\textbf{0.15$\\pm$0.03} &     \\textbf{0.15$\\pm$0.03} &     \\textbf{0.15$\\pm$0.03} &  \\underline{0.16$\\pm$0.03} \\\\\n",
      "     1461 &      \\textbf{0.11$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.11$\\pm$0.0} &      \\textbf{0.11$\\pm$0.0} &      \\textbf{0.11$\\pm$0.0} &                        nan &      \\textbf{0.11$\\pm$0.0} &      \\textbf{0.11$\\pm$0.0} \\\\\n",
      "     1464 &  \\underline{0.22$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.22$\\pm$0.02} &     \\textbf{0.21$\\pm$0.02} &     \\textbf{0.21$\\pm$0.03} &  \\underline{0.22$\\pm$0.03} &     \\textbf{0.21$\\pm$0.02} &     \\textbf{0.21$\\pm$0.03} \\\\\n",
      "     1468 &  \\underline{0.05$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.04$\\pm$0.02} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &  \\underline{0.05$\\pm$0.02} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} \\\\\n",
      "     1475 &                                                                          0.39$\\pm$0.03 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} &     \\textbf{0.37$\\pm$0.02} \\\\\n",
      "     1485 &                     \\textbf{0.11$\\pm$0.02} $\\dagger$ $\\dagger$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.26$\\pm$0.03 &              0.26$\\pm$0.03 &  \\underline{0.12$\\pm$0.02} &  \\underline{0.12$\\pm$0.02} &  \\underline{0.12$\\pm$0.02} &  \\underline{0.12$\\pm$0.01} \\\\\n",
      "     1486 &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &                        nan &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} \\\\\n",
      "     1487 &     \\textbf{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &  \\underline{0.06$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} \\\\\n",
      "     1489 &   \\underline{0.1$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} \\\\\n",
      "     1494 &                                                      0.14$\\pm$0.03 \\phantom{$\\circ$} $\\circ$ $\\circ$ \\phantom{$\\circ$} $\\circ$ $\\circ$ &              0.13$\\pm$0.03 &  \\underline{0.12$\\pm$0.03} &     \\textbf{0.11$\\pm$0.03} &  \\underline{0.12$\\pm$0.03} &  \\underline{0.12$\\pm$0.04} &  \\underline{0.12$\\pm$0.03} \\\\\n",
      "     1515 &          \\underline{0.07$\\pm$0.03} $\\bullet$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.13$\\pm$0.03 &     \\textbf{0.06$\\pm$0.02} &     \\textbf{0.06$\\pm$0.02} &  \\underline{0.07$\\pm$0.02} &  \\underline{0.07$\\pm$0.02} &  \\underline{0.07$\\pm$0.26} \\\\\n",
      "     1590 &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.16$\\pm$0.0} &      \\textbf{0.16$\\pm$0.0} &      \\textbf{0.16$\\pm$0.0} &                        nan &      \\textbf{0.16$\\pm$0.0} &      \\textbf{0.16$\\pm$0.0} \\\\\n",
      "     4134 &   \\underline{0.2$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.19$\\pm$0.02} &     \\textbf{0.19$\\pm$0.02} &     \\textbf{0.19$\\pm$0.01} &     \\textbf{0.19$\\pm$0.02} &   \\underline{0.2$\\pm$0.02} &   \\underline{0.2$\\pm$0.02} \\\\\n",
      "     4135 &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &                        nan &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} \\\\\n",
      "     4534 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &                        nan &     \\textbf{0.02$\\pm$0.01} &      \\textbf{0.02$\\pm$0.0} \\\\\n",
      "     4538 &  \\underline{0.31$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} \\\\\n",
      "     4541 &      \\textbf{0.42$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.42$\\pm$0.0} &      \\textbf{0.42$\\pm$0.0} &      \\textbf{0.42$\\pm$0.0} &                        nan &      \\textbf{0.42$\\pm$0.0} &      \\textbf{0.42$\\pm$0.0} \\\\\n",
      "    23512 &      \\textbf{0.28$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.28$\\pm$0.01} &     \\textbf{0.28$\\pm$0.01} &      \\textbf{0.28$\\pm$0.0} &     \\textbf{0.28$\\pm$0.01} &     \\textbf{0.28$\\pm$0.01} &     \\textbf{0.28$\\pm$0.01} \\\\\n",
      "    23517 &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.48$\\pm$0.0} &      \\textbf{0.48$\\pm$0.0} &     \\textbf{0.48$\\pm$0.01} &      \\textbf{0.48$\\pm$0.0} &      \\textbf{0.48$\\pm$0.0} &      \\textbf{0.48$\\pm$0.0} \\\\\n",
      "    40498 &                                                                          0.34$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &     \\textbf{0.29$\\pm$0.01} &   \\underline{0.3$\\pm$0.02} &   \\underline{0.3$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &   \\underline{0.3$\\pm$0.02} &     \\textbf{0.29$\\pm$0.01} \\\\\n",
      "    40668 &                                                          0.17$\\pm$0.06 $\\dagger$ $\\dagger$ $\\dagger$ $\\circ$ $\\circ$ \\phantom{$\\circ$} &               0.26$\\pm$0.0 &               0.26$\\pm$0.0 &              0.23$\\pm$0.07 &                        nan &      \\textbf{0.03$\\pm$0.0} &     \\textbf{0.03$\\pm$0.01} \\\\\n",
      "    40670 &     \\textbf{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &                        nan &     \\textbf{0.03$\\pm$0.02} &     \\textbf{0.03$\\pm$0.01} \\\\\n",
      "    40685 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "    40701 &      \\textbf{0.04$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &                        nan &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} \\\\\n",
      "    40900 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    40975 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.01$\\pm$0.01} &  \\underline{0.01$\\pm$0.01} &       \\textbf{0.0$\\pm$0.0} &                        nan &  \\underline{0.01$\\pm$0.01} &  \\underline{0.01$\\pm$0.01} \\\\\n",
      "    40978 &     \\textbf{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} &                        nan &     \\textbf{0.03$\\pm$0.01} &     \\textbf{0.03$\\pm$0.01} \\\\\n",
      "    40981 &     \\textbf{0.12$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.12$\\pm$0.03} &  \\underline{0.13$\\pm$0.04} &     \\textbf{0.12$\\pm$0.03} &                        nan &     \\textbf{0.12$\\pm$0.02} &     \\textbf{0.12$\\pm$0.02} \\\\\n",
      "    40982 &          \\underline{0.21$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\dagger$ \\phantom{$\\circ$} &  \\underline{0.21$\\pm$0.02} &  \\underline{0.22$\\pm$0.02} &      \\textbf{0.2$\\pm$0.03} &  \\underline{0.21$\\pm$0.02} &  \\underline{0.21$\\pm$0.02} &               0.25$\\pm$0.1 \\\\\n",
      "    40983 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    40984 &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.02$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &  \\underline{0.03$\\pm$0.02} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "    41027 &                                                     \\textbf{0.07$\\pm$0.01} $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ &              0.16$\\pm$0.01 &               0.14$\\pm$0.0 &               0.14$\\pm$0.0 &               0.14$\\pm$0.0 &               0.14$\\pm$0.0 &               0.14$\\pm$0.0 \\\\\n",
      "    41138 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    41142 &          \\underline{0.27$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\dagger$ \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.28$\\pm$0.02} &  \\underline{0.27$\\pm$0.02} &     \\textbf{0.26$\\pm$0.01} &                        nan &  \\underline{0.29$\\pm$0.03} &              0.28$\\pm$0.02 \\\\\n",
      "    41143 &             \\textbf{0.18$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.19$\\pm$0.02} &  \\underline{0.19$\\pm$0.03} &               0.2$\\pm$0.03 &                        nan &  \\underline{0.19$\\pm$0.03} &   \\underline{0.2$\\pm$0.04} \\\\\n",
      "    41144 &                      \\textbf{0.1$\\pm$0.02} $\\dagger$ $\\dagger$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &              0.24$\\pm$0.02 &              0.23$\\pm$0.01 &      \\textbf{0.1$\\pm$0.02} &  \\underline{0.11$\\pm$0.02} &  \\underline{0.11$\\pm$0.02} &  \\underline{0.11$\\pm$0.02} \\\\\n",
      "    41145 &                                                     \\textbf{0.18$\\pm$0.01} $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ &              0.25$\\pm$0.02 &              0.25$\\pm$0.01 &              0.25$\\pm$0.01 &              0.24$\\pm$0.01 &              0.25$\\pm$0.02 &              0.24$\\pm$0.01 \\\\\n",
      "    41146 &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} \\\\\n",
      "    41147 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.32$\\pm$0.0} &      \\textbf{0.32$\\pm$0.0} &                        nan &                        nan &                        nan &                        nan \\\\\n",
      "    41150 &      \\textbf{0.06$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.07$\\pm$0.0} &      \\textbf{0.06$\\pm$0.0} &   \\underline{0.07$\\pm$0.0} &      \\textbf{0.06$\\pm$0.0} &   \\underline{0.07$\\pm$0.0} &      \\textbf{0.06$\\pm$0.0} \\\\\n",
      "    41156 &     \\textbf{0.14$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.14$\\pm$0.01} &     \\textbf{0.14$\\pm$0.01} &     \\textbf{0.14$\\pm$0.01} &     \\textbf{0.14$\\pm$0.01} &     \\textbf{0.14$\\pm$0.01} &     \\textbf{0.14$\\pm$0.01} \\\\\n",
      "    41157 &                                  0.24$\\pm$0.09 $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.11$\\pm$0.09} &  \\underline{0.15$\\pm$0.08} &               0.21$\\pm$0.1 &              0.21$\\pm$0.05 &              0.26$\\pm$0.13 &              0.26$\\pm$0.15 \\\\\n",
      "    41158 &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} \\\\\n",
      "    41159 &     \\textbf{0.17$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.18$\\pm$0.01} &  \\underline{0.18$\\pm$0.01} &                        nan &                        nan &                        nan &                        nan \\\\\n",
      "    41162 &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &                        nan &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} \\\\\n",
      "    41163 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} &   \\underline{0.02$\\pm$0.0} \\\\\n",
      "    41164 &                                                                          0.31$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} &     \\textbf{0.29$\\pm$0.01} \\\\\n",
      "    41165 &     \\textbf{0.55$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.55$\\pm$0.01} &  \\underline{0.56$\\pm$0.01} &     \\textbf{0.55$\\pm$0.02} &  \\underline{0.56$\\pm$0.01} &     \\textbf{0.55$\\pm$0.01} &  \\underline{0.56$\\pm$0.01} \\\\\n",
      "    41166 &                                                      \\textbf{0.28$\\pm$0.0} $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ $\\dagger$ &               0.33$\\pm$0.0 &               0.32$\\pm$0.0 &               0.31$\\pm$0.0 &               0.32$\\pm$0.0 &               0.32$\\pm$0.0 &               0.32$\\pm$0.0 \\\\\n",
      "    41167 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.31$\\pm$0.0} &                        nan &                        nan &       \\textbf{0.2$\\pm$0.0} &                        nan &                        nan \\\\\n",
      "    41168 &                                                      \\textbf{0.27$\\pm$0.0} $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ &              0.29$\\pm$0.01 &               0.29$\\pm$0.0 &               0.29$\\pm$0.0 &              0.29$\\pm$0.01 &               0.29$\\pm$0.0 &               0.29$\\pm$0.0 \\\\\n",
      "    41169 &                                                                          0.69$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &   \\underline{0.65$\\pm$0.0} &      \\textbf{0.64$\\pm$0.0} &      \\textbf{0.64$\\pm$0.0} &      \\textbf{0.64$\\pm$0.0} &      \\textbf{0.64$\\pm$0.0} &      \\textbf{0.64$\\pm$0.0} \\\\\n",
      "    42732 &      \\textbf{0.12$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &                        nan &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} \\\\\n",
      "    42733 &      \\textbf{0.17$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.17$\\pm$0.0} &      \\textbf{0.17$\\pm$0.0} &      \\textbf{0.17$\\pm$0.0} &                        nan &      \\textbf{0.17$\\pm$0.0} &      \\textbf{0.17$\\pm$0.0} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_python = ((dfResults[\"algorithm\"].str.contains(\"python\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")))\n",
    "filter_python = filter_python & ~dfResults[\"algorithm\"].str.contains(\"wrapping\")\n",
    "filter_python = filter_python | (dfResults[\"algorithm\"] == \"auto-sklearn\")\n",
    "filter_monotone = (dfResults[\"algorithm\"].str.contains(\"monotone\"))\n",
    "filter_results = (dfResults[\"algorithm\"] == \"auto-sklearn\")\n",
    "filter_results = filter_results | (filter_python & filter_monotone)\n",
    "filter_results = filter_results | (filter_python & (dfResults[\"algorithm\"].str.contains(\"primitive\")))\n",
    "filter_results = filter_results | (filter_python & (dfResults[\"algorithm\"].str.contains(\"scaling\")))\n",
    "\n",
    "dfResultsMonotone = getResultTable(dfResults[filter_results], \"openmlid\", \"algorithm\", \"errorrate\", col_labels=col_labels_python, row_formatter=row_layouter)[[\"openmlid\", \"asklearn\", \"primitive\", \"scaling\", \"filtering\", \"meta\", \"tuning\", \"full\"]]\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    print(dfResultsMonotone.to_latex(index=False, escape=False).replace(\"{rlllllll}\", \"{rrrrrrrr}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2958: UserWarning: Exact p-value calculation does not work if there are ties. Switching to normal approximation.\n",
      "  warnings.warn(\"Exact p-value calculation does not work if there are \"\n",
      "/home/felix/anaconda3/lib/python3.7/site-packages/scipy/stats/morestats.py:2972: UserWarning: Sample size too small for normal approximation.\n",
      "  warnings.warn(\"Sample size too small for normal approximation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrrrrrr}\n",
      "\\toprule\n",
      " openmlid &                                                                                                            auto-weka &                                                                                                               mlplan &                  primitive &                  filtering &                       meta &                     tuning &                       full \\\\\n",
      "\\midrule\n",
      "        3 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.01$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &      \\textbf{0.0$\\pm$0.01} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "       12 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.04$\\pm$0.01 \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} \\\\\n",
      "       54 &  \\underline{0.18$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.18$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.17$\\pm$0.02} &     \\textbf{0.17$\\pm$0.02} &  \\underline{0.18$\\pm$0.02} &     \\textbf{0.17$\\pm$0.02} &  \\underline{0.18$\\pm$0.05} \\\\\n",
      "      181 &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.39$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.38$\\pm$0.03} &     \\textbf{0.38$\\pm$0.02} &  \\underline{0.39$\\pm$0.02} &  \\underline{0.39$\\pm$0.02} &  \\underline{0.39$\\pm$0.04} \\\\\n",
      "     1049 &   \\underline{0.1$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.02} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.02} &     \\textbf{0.09$\\pm$0.02} &     \\textbf{0.09$\\pm$0.02} \\\\\n",
      "     1067 &  \\underline{0.15$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.15$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.15$\\pm$0.02} &     \\textbf{0.14$\\pm$0.02} &     \\textbf{0.14$\\pm$0.02} &  \\underline{0.15$\\pm$0.02} &  \\underline{0.15$\\pm$0.01} \\\\\n",
      "     1111 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                              \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &               0.04$\\pm$0.0 &               0.04$\\pm$0.0 &               0.04$\\pm$0.0 \\\\\n",
      "     1457 &   \\underline{0.29$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.25$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.25$\\pm$0.03} &  \\underline{0.25$\\pm$0.04} &  \\underline{0.25$\\pm$0.03} &     \\textbf{0.24$\\pm$0.03} &     \\textbf{0.24$\\pm$0.03} \\\\\n",
      "     1461 &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.1$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} \\\\\n",
      "     1464 &  \\underline{0.22$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                        0.24$\\pm$0.01 $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.21$\\pm$0.04} &  \\underline{0.22$\\pm$0.03} &  \\underline{0.22$\\pm$0.03} &              0.23$\\pm$0.03 &              0.23$\\pm$0.03 \\\\\n",
      "     1468 &  \\underline{0.06$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} \\\\\n",
      "     1475 &              0.38$\\pm$0.03 \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.36$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.37$\\pm$0.02} &  \\underline{0.37$\\pm$0.02} &  \\underline{0.37$\\pm$0.02} &  \\underline{0.37$\\pm$0.08} &  \\underline{0.37$\\pm$0.02} \\\\\n",
      "     1485 &                                                                 0.39$\\pm$0.1 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                                                      0.23$\\pm$0.05 \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ $\\circ$ &              0.23$\\pm$0.02 &     \\textbf{0.11$\\pm$0.02} &     \\textbf{0.11$\\pm$0.01} &     \\textbf{0.11$\\pm$0.01} &     \\textbf{0.11$\\pm$0.02} \\\\\n",
      "     1486 &      \\textbf{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                            0.05$\\pm$0.01 $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ &      \\textbf{0.03$\\pm$0.0} &      \\textbf{0.03$\\pm$0.0} &  \\underline{0.04$\\pm$0.01} &  \\underline{0.04$\\pm$0.01} &      \\textbf{0.03$\\pm$0.0} \\\\\n",
      "     1487 &     \\textbf{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.06$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.05$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} &  \\underline{0.06$\\pm$0.01} &  \\underline{0.06$\\pm$0.01} &     \\textbf{0.05$\\pm$0.01} \\\\\n",
      "     1489 &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} \\\\\n",
      "     1494 &  \\underline{0.13$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.02} &  \\underline{0.13$\\pm$0.03} &              0.14$\\pm$0.03 &  \\underline{0.13$\\pm$0.03} &     \\textbf{0.12$\\pm$0.03} \\\\\n",
      "     1515 &     \\textbf{0.09$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.04} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.13$\\pm$0.04} &  \\underline{0.13$\\pm$0.04} &  \\underline{0.13$\\pm$0.04} &  \\underline{0.13$\\pm$0.04} &  \\underline{0.13$\\pm$0.04} \\\\\n",
      "     1590 &      \\textbf{0.14$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.14$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.14$\\pm$0.0} &      \\textbf{0.14$\\pm$0.0} &      \\textbf{0.14$\\pm$0.0} &      \\textbf{0.14$\\pm$0.0} &     \\textbf{0.14$\\pm$0.01} \\\\\n",
      "     4134 &  \\underline{0.23$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.2$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.2$\\pm$0.01} &   \\underline{0.2$\\pm$0.02} &     \\textbf{0.19$\\pm$0.02} &              0.21$\\pm$0.05 &              0.22$\\pm$0.05 \\\\\n",
      "     4135 &      \\textbf{0.05$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.06$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &      \\textbf{0.05$\\pm$0.0} &              0.08$\\pm$0.03 \\\\\n",
      "     4534 &   \\underline{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                                0.04$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} &      \\textbf{0.02$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "     4538 &  \\underline{0.32$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.32$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.32$\\pm$0.01} &  \\underline{0.32$\\pm$0.01} &  \\underline{0.32$\\pm$0.01} &  \\underline{0.32$\\pm$0.01} &     \\textbf{0.31$\\pm$0.01} \\\\\n",
      "     4541 &   \\underline{0.43$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.42$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.42$\\pm$0.01} &     \\textbf{0.42$\\pm$0.01} &     \\textbf{0.42$\\pm$0.01} &      \\textbf{0.42$\\pm$0.0} &     \\textbf{0.42$\\pm$0.01} \\\\\n",
      "    23512 &                                             0.31$\\pm$0.0 \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ &                                            0.32$\\pm$0.02 \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ &              0.31$\\pm$0.02 &              0.31$\\pm$0.01 &  \\underline{0.29$\\pm$0.01} &  \\underline{0.29$\\pm$0.01} &     \\textbf{0.28$\\pm$0.01} \\\\\n",
      "    23517 &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.48$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.48$\\pm$0.0} &   \\underline{0.49$\\pm$0.0} &     \\textbf{0.48$\\pm$0.01} &     \\textbf{0.48$\\pm$0.01} &     \\textbf{0.48$\\pm$0.01} \\\\\n",
      "    40498 &  \\underline{0.31$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.31$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} \\\\\n",
      "    40668 &                              \\textbf{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ &                                      0.19$\\pm$0.01 \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ &  \\underline{0.21$\\pm$0.11} &  \\underline{0.18$\\pm$0.12} &               0.27$\\pm$0.0 &               0.27$\\pm$0.0 &               0.27$\\pm$0.0 \\\\\n",
      "    40670 &     \\textbf{0.04$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.02} \\\\\n",
      "    40685 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} &       \\textbf{0.0$\\pm$0.0} \\\\\n",
      "    40701 &  \\underline{0.05$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.04$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} \\\\\n",
      "    40900 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    40975 &       \\textbf{0.0$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.0$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} &      \\textbf{0.0$\\pm$0.01} \\\\\n",
      "    40978 &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.03$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} \\\\\n",
      "    40981 &                                  0.14$\\pm$0.05 $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                  0.14$\\pm$0.03 $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.12$\\pm$0.03} &     \\textbf{0.12$\\pm$0.04} &  \\underline{0.13$\\pm$0.03} &              0.15$\\pm$0.03 &  \\underline{0.13$\\pm$0.03} \\\\\n",
      "    40982 &     \\textbf{0.19$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.2$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.2$\\pm$0.03} &   \\underline{0.2$\\pm$0.03} &   \\underline{0.2$\\pm$0.02} &   \\underline{0.2$\\pm$0.02} &  \\underline{0.21$\\pm$0.03} \\\\\n",
      "    40983 &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &      \\textbf{0.02$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "    40984 &     \\textbf{0.02$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                      0.04$\\pm$0.01 $\\circ$ $\\circ$ \\phantom{$\\circ$} $\\circ$ $\\circ$ &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &  \\underline{0.03$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "    41027 &                                                                0.16$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                                                                0.15$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &      \\textbf{0.09$\\pm$0.0} &      \\textbf{0.09$\\pm$0.0} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.01} &     \\textbf{0.09$\\pm$0.02} \\\\\n",
      "    41138 &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} &     \\textbf{0.01$\\pm$0.01} &      \\textbf{0.01$\\pm$0.0} &      \\textbf{0.01$\\pm$0.0} \\\\\n",
      "    41142 &                                                                0.29$\\pm$0.02 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                                                      0.28$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ $\\circ$ &              0.27$\\pm$0.02 &     \\textbf{0.25$\\pm$0.02} &  \\underline{0.26$\\pm$0.03} &  \\underline{0.26$\\pm$0.02} &  \\underline{0.26$\\pm$0.03} \\\\\n",
      "    41143 &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.17$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.17$\\pm$0.02} &  \\underline{0.17$\\pm$0.02} &  \\underline{0.17$\\pm$0.02} &  \\underline{0.17$\\pm$0.02} &  \\underline{0.18$\\pm$0.02} \\\\\n",
      "    41144 &  \\underline{0.27$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                      0.19$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ $\\circ$ &              0.21$\\pm$0.05 &              0.11$\\pm$0.01 &              0.11$\\pm$0.02 &   \\underline{0.1$\\pm$0.02} &     \\textbf{0.09$\\pm$0.01} \\\\\n",
      "    41145 &                                                                0.29$\\pm$0.03 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &                                                      0.26$\\pm$0.02 \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ $\\circ$ &              0.25$\\pm$0.01 &     \\textbf{0.13$\\pm$0.03} &  \\underline{0.14$\\pm$0.03} &     \\textbf{0.13$\\pm$0.03} &              0.16$\\pm$0.03 \\\\\n",
      "    41146 &      \\textbf{0.04$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                                0.07$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &  \\underline{0.05$\\pm$0.01} &  \\underline{0.05$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} &     \\textbf{0.04$\\pm$0.01} \\\\\n",
      "    41147 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.32$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.33$\\pm$0.01} &   \\underline{0.33$\\pm$0.0} &      \\textbf{0.32$\\pm$0.0} &      \\textbf{0.32$\\pm$0.0} &      \\textbf{0.32$\\pm$0.0} \\\\\n",
      "    41150 &      \\textbf{0.07$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.08$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.09$\\pm$0.0} &   \\underline{0.09$\\pm$0.0} &   \\underline{0.09$\\pm$0.0} &   \\underline{0.09$\\pm$0.0} &  \\underline{0.08$\\pm$0.01} \\\\\n",
      "    41156 &     \\textbf{0.15$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.15$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.15$\\pm$0.01} &     \\textbf{0.15$\\pm$0.01} \\\\\n",
      "    41157 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                0.21$\\pm$0.14 $\\circ$ \\phantom{$\\circ$} $\\bullet$ \\phantom{$\\circ$} \\phantom{$\\circ$} &     \\textbf{0.15$\\pm$0.09} &               0.3$\\pm$0.14 &              0.27$\\pm$0.12 &              0.26$\\pm$0.15 &              0.22$\\pm$0.11 \\\\\n",
      "    41158 &                          \\underline{0.07$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ &                          \\underline{0.07$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ &  \\underline{0.07$\\pm$0.04} &     \\textbf{0.06$\\pm$0.03} &              0.19$\\pm$0.02 &              0.19$\\pm$0.02 &              0.19$\\pm$0.05 \\\\\n",
      "    41159 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                             0.2$\\pm$0.01 \\phantom{$\\circ$} $\\circ$ $\\circ$ $\\circ$ \\phantom{$\\circ$} &              0.21$\\pm$0.01 &     \\textbf{0.17$\\pm$0.01} &     \\textbf{0.17$\\pm$0.01} &     \\textbf{0.17$\\pm$0.01} &  \\underline{0.18$\\pm$0.03} \\\\\n",
      "    41162 &       \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                               \\textbf{0.1$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ &       \\textbf{0.1$\\pm$0.0} &       \\textbf{0.1$\\pm$0.0} &  \\underline{0.23$\\pm$0.04} &  \\underline{0.17$\\pm$0.07} &  \\underline{0.22$\\pm$0.06} \\\\\n",
      "    41163 &   \\underline{0.08$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.03$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &   \\underline{0.03$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} &     \\textbf{0.02$\\pm$0.01} &   \\underline{0.03$\\pm$0.0} &     \\textbf{0.02$\\pm$0.01} \\\\\n",
      "    41164 &                                            0.35$\\pm$0.01 $\\circ$ $\\circ$ \\phantom{$\\circ$} \\phantom{$\\circ$} $\\circ$ &                                        0.32$\\pm$0.02 $\\circ$ \\phantom{$\\circ$} $\\bullet$ $\\bullet$ \\phantom{$\\circ$} &      \\textbf{0.3$\\pm$0.01} &  \\underline{0.31$\\pm$0.01} &              0.34$\\pm$0.01 &              0.34$\\pm$0.01 &              0.33$\\pm$0.02 \\\\\n",
      "    41165 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &  \\underline{0.62$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.6$\\pm$0.03} &  \\underline{0.61$\\pm$0.03} &                        nan &                        nan &                        nan \\\\\n",
      "    41166 &  \\underline{0.36$\\pm$0.03} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                      0.35$\\pm$0.01 $\\circ$ $\\circ$ $\\circ$ $\\circ$ \\phantom{$\\circ$} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &      \\textbf{0.3$\\pm$0.01} &               0.34$\\pm$0.1 \\\\\n",
      "    41167 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                                                 0.96$\\pm$0.0 $\\circ$ $\\circ$ $\\circ$ $\\circ$ $\\circ$ &              0.12$\\pm$0.07 &      \\textbf{0.09$\\pm$0.0} &      \\textbf{0.09$\\pm$0.0} &      \\textbf{0.09$\\pm$0.0} &      \\textbf{0.09$\\pm$0.0} \\\\\n",
      "    41168 &  \\underline{0.34$\\pm$0.01} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                                    \\underline{0.3$\\pm$0.0} $\\bullet$ $\\bullet$ $\\bullet$ $\\bullet$ \\phantom{$\\circ$} &              0.34$\\pm$0.01 &              0.34$\\pm$0.02 &               0.36$\\pm$0.0 &               0.36$\\pm$0.0 &     \\textbf{0.29$\\pm$0.01} \\\\\n",
      "    41169 &      \\textbf{0.71$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &                           \\underline{0.72$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ $\\bullet$ $\\bullet$ &   \\underline{0.72$\\pm$0.0} &   \\underline{0.72$\\pm$0.0} &  \\underline{0.75$\\pm$0.02} &  \\underline{0.75$\\pm$0.02} &  \\underline{0.75$\\pm$0.01} \\\\\n",
      "    42732 &                        nan \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.12$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} &      \\textbf{0.12$\\pm$0.0} \\\\\n",
      "    42733 &      \\textbf{0.16$\\pm$0.0} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} &          \\underline{0.17$\\pm$0.02} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} \\phantom{$\\circ$} $\\bullet$ &   \\underline{0.17$\\pm$0.0} &   \\underline{0.17$\\pm$0.0} &   \\underline{0.17$\\pm$0.0} &   \\underline{0.17$\\pm$0.0} &  \\underline{0.25$\\pm$0.03} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_weka = ((dfResults[\"algorithm\"].str.contains(\"java\")) & (dfResults[\"algorithm\"].str.contains(\"noniterative\")))\n",
    "filter_weka = filter_weka & ~dfResults[\"algorithm\"].str.contains(\"wrapping\")\n",
    "filter_monotone = (dfResults[\"algorithm\"].str.contains(\"monotone\"))\n",
    "filter_results = (dfResults[\"algorithm\"] == \"auto-weka\") | (dfResults[\"algorithm\"] == \"mlplan\")\n",
    "filter_results = filter_results | (filter_weka & filter_monotone)\n",
    "filter_results = filter_results | (filter_weka & (dfResults[\"algorithm\"].str.contains(\"primitive\")))\n",
    "filter_results = filter_results | (filter_weka & (dfResults[\"algorithm\"].str.contains(\"filtering\")))\n",
    "\n",
    "dfResultsMonotone = getResultTable(dfResults[filter_results], \"openmlid\", \"algorithm\", \"errorrate\", col_labels=col_labels_python, row_formatter=row_layouter)[[\"openmlid\", \"auto-weka\", \"mlplan\", \"primitive\", \"filtering\", \"meta\", \"tuning\", \"full\"]]\n",
    "with pd.option_context(\"max_colwidth\", 1000):\n",
    "    print(dfResultsMonotone.to_latex(index=False, escape=False).replace(\"{rlllllll}\", \"{rrrrrrrr}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtimes Per Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getPhaseRuntimeDF(df):\n",
    "    algorithms = pd.unique(df[\"algorithm\"])\n",
    "    stages = [\"probing\", \"scaling\", \"filtering\", \"meta\", \"tuning\", \"validation\"]\n",
    "    if len(algorithms) != 1:\n",
    "        raise Exception(\"The dataframe must contain entries for EXACTLY one algorithm\")\n",
    "    algorithm = algorithms[0]\n",
    "    rows = []\n",
    "    for openmlid, dfDataset in df.groupby(\"openmlid\"):\n",
    "        stage_times = {}\n",
    "        for seed, dfSeed in dfDataset.groupby(\"seed\"):\n",
    "            try:\n",
    "                json = ast.literal_eval(dfSeed[\"onlinedata\"].values[0])\n",
    "                for stage in json[\"stageruntimes\"]:\n",
    "                    if not stage in stage_times:\n",
    "                        stage_times[stage] = []\n",
    "                    if str(json[\"stageruntimes\"][stage]) != \"nan\":\n",
    "                        stage_times[stage].append(json[\"stageruntimes\"][stage])\n",
    "            except ValueError:\n",
    "                print(\"IGNORING ENTRY\")\n",
    "                pass\n",
    "        \n",
    "        row = [openmlid]\n",
    "        total = 0\n",
    "        for stage in stages:\n",
    "            if stage in stage_times:\n",
    "                stage_contribution = min(3600, int(np.round(scipy.stats.trim_mean(stage_times[stage], 0.1) / 1000)))\n",
    "                stage_contribution = min(3600 - total, stage_contribution)\n",
    "                row.append(stage_contribution)\n",
    "                total += stage_contribution\n",
    "            else:\n",
    "                row.append(0)\n",
    "        row.append(total)\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows, columns=[\"openmlid\"] + stages + [\"Total\"])\n",
    "        #print(openmlid, len(dfDataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEKA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "\\begin{tabular}{rrrrrrrr}\n",
      "\\toprule\n",
      " openmlid &  probing &  scaling &  filtering &  meta &  tuning &  validation &  Total \\\\\n",
      "\\midrule\n",
      "        3 &      123 &        0 &         65 &    80 &      28 &         127 &    423 \\\\\n",
      "       12 &      558 &        0 &        547 &    33 &      58 &         168 &   1364 \\\\\n",
      "       54 &       19 &        0 &         17 &    26 &      22 &          42 &    126 \\\\\n",
      "      181 &       27 &        0 &          6 &    11 &      15 &          26 &     85 \\\\\n",
      "     1049 &       62 &        0 &         44 &     8 &      21 &          41 &    176 \\\\\n",
      "     1067 &       53 &        0 &         34 &    21 &      19 &          42 &    169 \\\\\n",
      "     1111 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "     1457 &     1398 &        0 &       1085 &    50 &      10 &         299 &   2842 \\\\\n",
      "     1461 &      775 &        0 &        441 &    35 &      36 &         248 &   1535 \\\\\n",
      "     1464 &        2 &        0 &          3 &     3 &      19 &          12 &     39 \\\\\n",
      "     1468 &     2413 &        0 &        833 &     4 &      14 &         113 &   3377 \\\\\n",
      "     1475 &     1961 &        0 &        728 &    23 &      40 &         213 &   2965 \\\\\n",
      "     1485 &     1741 &        0 &        170 &     7 &      21 &          66 &   2005 \\\\\n",
      "     1486 &     2927 &        0 &        673 &     0 &       0 &           0 &   3600 \\\\\n",
      "     1487 &      353 &        0 &        195 &    30 &      22 &         174 &    774 \\\\\n",
      "     1489 &      191 &        0 &          1 &   943 &      74 &         101 &   1310 \\\\\n",
      "     1494 &       46 &        0 &         47 &    15 &      25 &          40 &    173 \\\\\n",
      "     1515 &     3569 &        0 &         31 &     0 &       0 &           0 &   3600 \\\\\n",
      "     1590 &     1662 &        0 &        446 &    32 &      25 &         335 &   2500 \\\\\n",
      "     4134 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "     4135 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "     4534 &      297 &        0 &        323 &    18 &      23 &         235 &    896 \\\\\n",
      "     4538 &      360 &        0 &        254 &    68 &      62 &         167 &    911 \\\\\n",
      "     4541 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    23512 &     1081 &        0 &        722 &   218 &     243 &         321 &   2585 \\\\\n",
      "    23517 &      942 &        0 &        422 &   255 &     122 &         308 &   2049 \\\\\n",
      "    40498 &      140 &        0 &         80 &     2 &      28 &          81 &    331 \\\\\n",
      "    40668 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    40670 &      446 &        0 &        173 &   118 &      47 &         119 &    903 \\\\\n",
      "    40685 &      671 &        0 &        639 &    19 &      19 &         440 &   1788 \\\\\n",
      "    40701 &      274 &        0 &        191 &    15 &      30 &         160 &    670 \\\\\n",
      "    40900 &      239 &        0 &         96 &    16 &      23 &         129 &    503 \\\\\n",
      "    40975 &       29 &        0 &         24 &     7 &      23 &          33 &    116 \\\\\n",
      "    40978 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    40981 &       17 &        0 &          5 &     2 &      20 &          15 &     59 \\\\\n",
      "    40982 &      384 &        0 &        101 &     1 &      28 &          78 &    592 \\\\\n",
      "    40983 &      184 &        0 &        122 &    38 &      42 &         138 &    524 \\\\\n",
      "    40984 &      129 &        0 &         73 &   166 &      27 &          76 &    471 \\\\\n",
      "    41027 &      520 &        0 &          3 &    36 &      57 &         393 &   1009 \\\\\n",
      "    41138 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41142 &     1278 &        0 &        654 &     5 &       4 &        1659 &   3600 \\\\\n",
      "    41143 &      320 &        0 &        226 &     4 &      26 &         120 &    696 \\\\\n",
      "    41144 &      652 &        0 &        212 &     2 &      19 &          65 &    950 \\\\\n",
      "    41145 &     1520 &        0 &        334 &     3 &      32 &         159 &   2048 \\\\\n",
      "    41146 &      190 &        0 &         84 &     1 &      44 &          99 &    418 \\\\\n",
      "    41147 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41150 &     2692 &        0 &        908 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41156 &      245 &        0 &        183 &    36 &      26 &         154 &    644 \\\\\n",
      "    41157 &      292 &        0 &        495 &     2 &       1 &          50 &    840 \\\\\n",
      "    41158 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41159 &     2493 &        0 &        676 &    42 &      32 &         193 &   3436 \\\\\n",
      "    41162 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41163 &     1210 &        0 &       1388 &    24 &      15 &         317 &   2954 \\\\\n",
      "    41164 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41165 &        0 &        0 &          0 &     0 &       0 &           0 &      0 \\\\\n",
      "    41166 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41167 &     2072 &        0 &       1528 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41168 &     3351 &        0 &        249 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41169 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    42732 &     1395 &        0 &       1091 &   322 &      26 &         766 &   3600 \\\\\n",
      "    42733 &     3600 &        0 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stageTimeTable = getPhaseRuntimeDF(dfResults.query(\"algorithm == 'naive-java-validation-noniterative-monotone'\"))\n",
    "print(stageTimeTable.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "IGNORING ENTRY\n",
      "\\begin{tabular}{rrrrrrrr}\n",
      "\\toprule\n",
      " openmlid &  probing &  scaling &  filtering &  meta &  tuning &  validation &  Total \\\\\n",
      "\\midrule\n",
      "        3 &       31 &       25 &         49 &  1122 &     510 &          21 &   1758 \\\\\n",
      "       12 &      202 &      772 &        544 &  2082 &       0 &           0 &   3600 \\\\\n",
      "       54 &        9 &       49 &         58 &  1230 &     373 &           1 &   1720 \\\\\n",
      "      181 &       18 &       75 &         91 &  2415 &     394 &          25 &   3018 \\\\\n",
      "     1049 &        6 &       49 &         48 &  2049 &     420 &          30 &   2602 \\\\\n",
      "     1067 &        7 &       52 &         54 &  3410 &      77 &           0 &   3600 \\\\\n",
      "     1111 &      746 &      381 &        234 &  2239 &       0 &           0 &   3600 \\\\\n",
      "     1457 &      602 &     1681 &       1204 &   113 &       0 &           0 &   3600 \\\\\n",
      "     1461 &      495 &       88 &        109 &  2332 &     511 &          22 &   3557 \\\\\n",
      "     1464 &        3 &       19 &         22 &  1727 &     324 &           9 &   2104 \\\\\n",
      "     1468 &       48 &      145 &         85 &  2567 &     332 &          46 &   3223 \\\\\n",
      "     1475 &      135 &      643 &        640 &  2182 &       0 &           0 &   3600 \\\\\n",
      "     1485 &       81 &      347 &         80 &  2917 &     139 &           4 &   3568 \\\\\n",
      "     1486 &      636 &      564 &        346 &  2054 &       0 &           0 &   3600 \\\\\n",
      "     1487 &       16 &       72 &         74 &  1474 &     446 &          29 &   2111 \\\\\n",
      "     1489 &       23 &       45 &         68 &  1918 &     359 &          12 &   2425 \\\\\n",
      "     1494 &        8 &       41 &         49 &   912 &     453 &          18 &   1481 \\\\\n",
      "     1515 &      138 &      710 &        218 &  2534 &       0 &           0 &   3600 \\\\\n",
      "     1590 &      503 &       78 &        204 &  2038 &     525 &          39 &   3387 \\\\\n",
      "     4134 &      201 &      913 &        149 &  2337 &       0 &           0 &   3600 \\\\\n",
      "     4135 &      465 &      269 &         83 &  2172 &     472 &         139 &   3600 \\\\\n",
      "     4534 &      108 &       99 &        116 &  2207 &     990 &          23 &   3543 \\\\\n",
      "     4538 &      181 &      884 &        944 &  1591 &       0 &           0 &   3600 \\\\\n",
      "     4541 &      811 &      480 &        403 &  1906 &       0 &           0 &   3600 \\\\\n",
      "    23512 &      823 &     2777 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    23517 &      531 &      765 &        329 &  1908 &       5 &           0 &   3538 \\\\\n",
      "    40498 &       42 &      255 &        207 &  3096 &       0 &           0 &   3600 \\\\\n",
      "    40668 &      714 &      585 &        265 &  2036 &       0 &           0 &   3600 \\\\\n",
      "    40670 &      108 &       47 &         53 &  2192 &     430 &         378 &   3208 \\\\\n",
      "    40685 &      267 &     1457 &       1539 &   337 &       0 &           0 &   3600 \\\\\n",
      "    40701 &       16 &      136 &         69 &  3371 &       8 &           0 &   3600 \\\\\n",
      "    40900 &       17 &       65 &         62 &  1258 &     371 &          11 &   1784 \\\\\n",
      "    40975 &       12 &       56 &         74 &  3458 &       0 &           0 &   3600 \\\\\n",
      "    40978 &      278 &      157 &         49 &  2702 &     332 &          53 &   3571 \\\\\n",
      "    40981 &        6 &       26 &         25 &  1134 &     407 &           9 &   1607 \\\\\n",
      "    40982 &       32 &      179 &        100 &  3289 &       0 &           0 &   3600 \\\\\n",
      "    40983 &       12 &       74 &         79 &  1605 &     571 &          98 &   2439 \\\\\n",
      "    40984 &       33 &      154 &        174 &  3239 &       0 &           0 &   3600 \\\\\n",
      "    41027 &      217 &     1017 &       1220 &  1146 &       0 &           0 &   3600 \\\\\n",
      "    41138 &      848 &     2752 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41142 &      291 &     1359 &        230 &  1720 &       0 &           0 &   3600 \\\\\n",
      "    41143 &       77 &       58 &         35 &  1206 &    1262 &         104 &   2742 \\\\\n",
      "    41144 &       55 &      354 &        115 &  3076 &       0 &           0 &   3600 \\\\\n",
      "    41145 &      196 &     1064 &        147 &  2193 &       0 &           0 &   3600 \\\\\n",
      "    41146 &       23 &      133 &        113 &  3331 &       0 &           0 &   3600 \\\\\n",
      "    41147 &        0 &        0 &          0 &     0 &       0 &           0 &      0 \\\\\n",
      "    41150 &      739 &     2607 &        254 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41156 &       12 &      103 &         60 &  3209 &     216 &           0 &   3600 \\\\\n",
      "    41157 &       40 &      184 &         63 &  1453 &     416 &          11 &   2167 \\\\\n",
      "    41158 &       96 &      555 &        256 &  2693 &       0 &           0 &   3600 \\\\\n",
      "    41159 &        0 &        0 &          0 &     0 &       0 &           0 &      0 \\\\\n",
      "    41162 &      638 &      339 &        170 &  2453 &       0 &           0 &   3600 \\\\\n",
      "    41163 &      918 &     2682 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41164 &      472 &     1051 &        515 &  1562 &       0 &           0 &   3600 \\\\\n",
      "    41165 &     1428 &     2172 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41166 &      536 &     2449 &        615 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41167 &        0 &        0 &          0 &     0 &       0 &           0 &      0 \\\\\n",
      "    41168 &      747 &     2853 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    41169 &      836 &     2764 &          0 &     0 &       0 &           0 &   3600 \\\\\n",
      "    42732 &      871 &      495 &       1186 &  1048 &       0 &           0 &   3600 \\\\\n",
      "    42733 &      479 &      352 &        323 &  2446 &       0 &           0 &   3600 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stageTimeTable = getPhaseRuntimeDF(dfResults.query(\"algorithm == 'naive-python-validation-noniterative-monotone'\"))\n",
    "print(stageTimeTable.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      V1   V2           V3   V4   V5   V6            V7   V8   V9  V10  ...  \\\n",
      "0    0.0  0.0       0.0000  0.0  0.0  0.0  44431.921875  0.0  0.0  0.0  ...   \n",
      "1    0.0  0.0       0.0000  0.0  0.0  0.0      0.000000  0.0  0.0  0.0  ...   \n",
      "2    0.0  0.0       0.0000  0.0  0.0  0.0      0.000000  0.0  0.0  0.0  ...   \n",
      "3    0.0  0.0       0.0000  0.0  0.0  0.0      0.000000  0.0  0.0  0.0  ...   \n",
      "4    0.0  0.0       0.0000  0.0  0.0  0.0   4701.712402  0.0  0.0  0.0  ...   \n",
      "..   ...  ...          ...  ...  ...  ...           ...  ...  ...  ...  ...   \n",
      "566  0.0  0.0       0.0000  0.0  0.0  0.0      0.000000  0.0  0.0  0.0  ...   \n",
      "567  0.0  0.0       0.0000  0.0  0.0  0.0      0.000000  0.0  0.0  0.0  ...   \n",
      "568  0.0  0.0       0.0000  0.0  0.0  0.0      0.000000  0.0  0.0  0.0  ...   \n",
      "569  0.0  0.0  145578.5625  0.0  0.0  0.0      0.000000  0.0  0.0  0.0  ...   \n",
      "570  0.0  0.0  626135.8125  0.0  0.0  0.0      0.000000  0.0  0.0  0.0  ...   \n",
      "\n",
      "     V1292  V1293  V1294         V1295  V1296  V1297  V1298         V1299  \\\n",
      "0      0.0    0.0    0.0      0.000000    0.0    0.0    0.0      0.000000   \n",
      "1      0.0    0.0    0.0  35438.351563    0.0    0.0    0.0      0.000000   \n",
      "2      0.0    0.0    0.0   6384.590820    0.0    0.0    0.0      0.000000   \n",
      "3      0.0    0.0    0.0      0.000000    0.0    0.0    0.0      0.000000   \n",
      "4      0.0    0.0    0.0      0.000000    0.0    0.0    0.0      0.000000   \n",
      "..     ...    ...    ...           ...    ...    ...    ...           ...   \n",
      "566    0.0    0.0    0.0      0.000000    0.0    0.0    0.0      0.000000   \n",
      "567    0.0    0.0    0.0      0.000000    0.0    0.0    0.0      0.000000   \n",
      "568    0.0    0.0    0.0      0.000000    0.0    0.0    0.0  24808.794922   \n",
      "569    0.0    0.0    0.0      0.000000    0.0    0.0    0.0      0.000000   \n",
      "570    0.0    0.0    0.0      0.000000    0.0    0.0    0.0      0.000000   \n",
      "\n",
      "     V1300  Class  \n",
      "0      0.0     10  \n",
      "1      0.0     17  \n",
      "2      0.0     17  \n",
      "3      0.0     17  \n",
      "4      0.0     17  \n",
      "..     ...    ...  \n",
      "566    0.0     13  \n",
      "567    0.0     13  \n",
      "568    0.0     13  \n",
      "569    0.0      5  \n",
      "570    0.0      5  \n",
      "\n",
      "[571 rows x 1301 columns]\n"
     ]
    }
   ],
   "source": [
    "ds = openml.datasets.get_dataset(1515)\n",
    "df = ds.get_data()[0]\n",
    "print(df)\n",
    "y = df[ds.default_target_attribute].values\n",
    "X = df[[c for c in df.columns if c != ds.default_target_attribute]].values\n",
    "timeout = 60 * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataset(openmlid):\n",
    "    ds = openml.datasets.get_dataset(openmlid)\n",
    "    df = ds.get_data()[0]\n",
    "    y = df[ds.default_target_attribute].values\n",
    "    X = pd.get_dummies(df[[c for c in df.columns if c != ds.default_target_attribute]]).values.astype(float)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a372fd9a91e4799bc8b2a00dd310065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:openml.datasets.dataset:Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1457 0\n",
      "1350 training instances and 150 test instances.\n",
      "[DEBUG] [2021-02-03 02:00:55,285:asyncio] Using selector: EpollSelector\n",
      "[DEBUG] [2021-02-03 02:00:55,287:asyncio] Using selector: EpollSelector\n",
      "[INFO] [2021-02-03 02:01:01,921:smac.utils.io.cmd_reader.CMDReader] Output to /tmp/autosklearn_tmp_43ba37b8-65bb-11eb-9aa6-3315768f62dd/smac3-output\n",
      "[DEBUG] [2021-02-03 02:01:01,922:smac.scenario.scenario.Scenario] SMAC and Scenario Options:\n",
      "[INFO] [2021-02-03 02:01:01,925:smac.facade.smac_ac_facade.SMAC4AC] Optimizing a deterministic scenario for quality without a tuner timeout - will make SMAC deterministic and only evaluate one configuration per iteration!\n",
      "[DEBUG] [2021-02-03 02:01:01,926:smac.scenario.scenario.Scenario] Output directory does not exist! Will be created.\n",
      "[DEBUG] [2021-02-03 02:01:01,927:smac.scenario.scenario.Scenario] Writing scenario-file to /tmp/autosklearn_tmp_43ba37b8-65bb-11eb-9aa6-3315768f62dd/smac3-output/run_1/scenario.txt.\n",
      "[INFO] [2021-02-03 02:01:02,029:smac.initial_design.initial_design.InitialDesign] Running initial design for 26 configurations\n",
      "[INFO] [2021-02-03 02:01:02,031:smac.optimizer.smbo.SMBO] Running initial design\n",
      "[INFO] [2021-02-03 02:01:02,031:smac.optimizer.smbo.SMBO] Running initial design\n",
      "[DEBUG] [2021-02-03 02:01:02,036:smac.intensification.simple_intensifier.SimpleIntensifier] Using challengers provided\n",
      "[DEBUG] [2021-02-03 02:01:02,036:smac.intensification.simple_intensifier.SimpleIntensifier] Time to select next challenger: 0.0004\n",
      "[DEBUG] [2021-02-03 02:01:02,055:smac.intensification.simple_intensifier.SimpleIntensifier] Using challengers provided\n",
      "[DEBUG] [2021-02-03 02:01:02,055:smac.intensification.simple_intensifier.SimpleIntensifier] Time to select next challenger: 0.0005\n",
      "[INFO] [2021-02-03 02:01:10,163:smac.intensification.simple_intensifier.SimpleIntensifier] First run, no incumbent provided; challenger is assumed to be the incumbent\n",
      "[DEBUG] [2021-02-03 02:01:10,167:smac.intensification.simple_intensifier.SimpleIntensifier] Incumbent (1.0000) is at least as good as the challenger (1.0000) on 1 runs.\n",
      "[DEBUG] [2021-02-03 02:01:10,173:smac.intensification.simple_intensifier.SimpleIntensifier] Using challengers provided\n",
      "[DEBUG] [2021-02-03 02:01:10,174:smac.intensification.simple_intensifier.SimpleIntensifier] Time to select next challenger: 0.0011\n",
      "[DEBUG] [2021-02-03 02:01:17,985:smac.intensification.simple_intensifier.SimpleIntensifier] Incumbent (1.0000) is at least as good as the challenger (1.0000) on 1 runs.\n",
      "[DEBUG] [2021-02-03 02:01:17,991:smac.intensification.simple_intensifier.SimpleIntensifier] Using challengers provided\n",
      "[DEBUG] [2021-02-03 02:01:17,992:smac.intensification.simple_intensifier.SimpleIntensifier] Time to select next challenger: 0.0010\n",
      "[DEBUG] [2021-02-03 02:01:26,604:smac.intensification.simple_intensifier.SimpleIntensifier] Incumbent (1.0000) is at least as good as the challenger (1.0000) on 1 runs.\n",
      "[DEBUG] [2021-02-03 02:01:26,621:smac.intensification.simple_intensifier.SimpleIntensifier] Using challengers provided\n",
      "[DEBUG] [2021-02-03 02:01:26,622:smac.intensification.simple_intensifier.SimpleIntensifier] Time to select next challenger: 0.0009\n",
      "[DEBUG] [2021-02-03 02:01:34,218:smac.intensification.simple_intensifier.SimpleIntensifier] Incumbent (1.0000) is at least as good as the challenger (1.0000) on 1 runs.\n",
      "[DEBUG] [2021-02-03 02:01:34,222:smac.intensification.simple_intensifier.SimpleIntensifier] Using challengers provided\n",
      "[DEBUG] [2021-02-03 02:01:34,223:smac.intensification.simple_intensifier.SimpleIntensifier] Time to select next challenger: 0.0011\n",
      "[DEBUG] [2021-02-03 02:01:42,915:smac.intensification.simple_intensifier.SimpleIntensifier] Incumbent (1.0000) is at least as good as the challenger (1.0000) on 1 runs.\n",
      "[DEBUG] [2021-02-03 02:01:42,927:smac.intensification.simple_intensifier.SimpleIntensifier] Using challengers provided\n",
      "[DEBUG] [2021-02-03 02:01:42,928:smac.intensification.simple_intensifier.SimpleIntensifier] Time to select next challenger: 0.0010\n",
      "[DEBUG] [2021-02-03 02:01:45,590:smac.intensification.simple_intensifier.SimpleIntensifier] Incumbent (1.0000) is at least as good as the challenger (1.0000) on 1 runs.\n",
      "[DEBUG] [2021-02-03 02:01:45,595:smac.intensification.simple_intensifier.SimpleIntensifier] Using challengers provided\n",
      "[DEBUG] [2021-02-03 02:01:45,595:smac.intensification.simple_intensifier.SimpleIntensifier] Time to select next challenger: 0.0013\n",
      "[DEBUG] [2021-02-03 02:01:46,707:smac.stats.stats.Stats] Saving stats to /tmp/autosklearn_tmp_43ba37b8-65bb-11eb-9aa6-3315768f62dd/smac3-output/run_1/stats.json\n",
      "[INFO] [2021-02-03 02:01:46,709:smac.stats.stats.Stats] ##########################################################\n",
      "[INFO] [2021-02-03 02:01:46,710:smac.stats.stats.Stats] Statistics:\n",
      "[INFO] [2021-02-03 02:01:46,710:smac.stats.stats.Stats] #Incumbent changed: 0\n",
      "[INFO] [2021-02-03 02:01:46,711:smac.stats.stats.Stats] #Submitted target algorithm runs: 8 / inf\n",
      "[INFO] [2021-02-03 02:01:46,711:smac.stats.stats.Stats] #Finished target algorithm runs: 8 / inf\n",
      "[INFO] [2021-02-03 02:01:46,712:smac.stats.stats.Stats] #Configurations: 7\n",
      "[INFO] [2021-02-03 02:01:46,712:smac.stats.stats.Stats] Used wallclock time: 44.68 / 47.06 sec \n",
      "[INFO] [2021-02-03 02:01:46,713:smac.stats.stats.Stats] Used target algorithm runtime: 34.33 / inf sec\n",
      "[DEBUG] [2021-02-03 02:01:46,713:smac.stats.stats.Stats] Debug Statistics:\n",
      "[INFO] [2021-02-03 02:01:46,714:smac.stats.stats.Stats] ##########################################################\n",
      "[INFO] [2021-02-03 02:01:46,714:smac.facade.smac_ac_facade.SMAC4AC] Final Incumbent: Configuration:\n",
      "  balancing:strategy, Value: 'none'\n",
      "  classifier:__choice__, Value: 'random_forest'\n",
      "  classifier:random_forest:bootstrap, Value: 'True'\n",
      "  classifier:random_forest:criterion, Value: 'gini'\n",
      "  classifier:random_forest:max_depth, Constant: 'None'\n",
      "  classifier:random_forest:max_features, Value: 0.5\n",
      "  classifier:random_forest:max_leaf_nodes, Constant: 'None'\n",
      "  classifier:random_forest:min_impurity_decrease, Constant: 0.0\n",
      "  classifier:random_forest:min_samples_leaf, Value: 1\n",
      "  classifier:random_forest:min_samples_split, Value: 2\n",
      "  classifier:random_forest:min_weight_fraction_leaf, Constant: 0.0\n",
      "  data_preprocessing:categorical_transformer:categorical_encoding:__choice__, Value: 'one_hot_encoding'\n",
      "  data_preprocessing:categorical_transformer:category_coalescence:__choice__, Value: 'minority_coalescer'\n",
      "  data_preprocessing:categorical_transformer:category_coalescence:minority_coalescer:minimum_fraction, Value: 0.01\n",
      "  data_preprocessing:numerical_transformer:imputation:strategy, Value: 'mean'\n",
      "  data_preprocessing:numerical_transformer:rescaling:__choice__, Value: 'standardize'\n",
      "  feature_preprocessor:__choice__, Value: 'no_preprocessing'\n",
      "\n",
      "[INFO] [2021-02-03 02:01:46,718:smac.facade.smac_ac_facade.SMAC4AC] Estimated cost of incumbent: 1.000000\n",
      "Accuracy score 0.9666666666666667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets_suite_271 = [\n",
    "    1590, 1515, 1457, 1475, 1468, 1486, 1489, 23512, 23517, 4541, 4534, 4538, 4134, \n",
    "    4135, 40978, 40996, 41027, 40981, 40982, 40983, 40984, 40701, 40670, 40685, 40900, \n",
    "    1111, 42732, 42733, 42734, 40498, 41161, 41162, 41163, 41164, 41165, 41166, 41167, \n",
    "    41168, 41169, 41142, 41143, 41144, 41145, 41146, 41147, 41150, 41156, 41157, 41158, \n",
    "    41159, 41138, 54, 181, 188, 1461, 1494, 1464, 12, 23, 3, 1487, 40668, 1067, 1049, \n",
    "    40975, 31 \n",
    "]\n",
    "\n",
    "timeout = 60 * 1\n",
    "\n",
    "datasets = [1457]#datasets_suite_271\n",
    "numseeds = 1\n",
    "\n",
    "pbar = tqdm(total=numseeds * len(datasets))\n",
    "\n",
    "rows = []\n",
    "cnt = 0\n",
    "for openmlid in datasets:\n",
    "    X, y = getDataset(openmlid)\n",
    "    for seed in range(numseeds):\n",
    "        print(openmlid, seed)\n",
    "\n",
    "\n",
    "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, train_size=0.9, random_state=seed)\n",
    "        print(str(len(X_train)) + \" training instances and \" + str(len(X_test)) + \" test instances.\")\n",
    "        automl = autosklearn.classification.AutoSklearnClassifier(time_left_for_this_task=timeout)\n",
    "        automl.fit(X_train, y_train)\n",
    "        y_hat = automl.predict(X_test)\n",
    "        score = 1 - sklearn.metrics.accuracy_score(y_test, y_hat)\n",
    "        print(\"Accuracy score\", score)\n",
    "        rows.append([openmlid, seed, score])\n",
    "        pbar.update(1)\n",
    "pbar.close()\n",
    "results_sklearn = pd.DataFrame(rows, columns=[\"openmlid\", \"seed\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] [2021-02-03 02:11:29,140:openml.datasets.dataset] Data pickle file already exists and is up to date.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.61666667, 0.53666667, 0.63666667, 0.56333333, 0.62666667])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst = sklearn.linear_model.PassiveAggressiveClassifier(C=0.013648712515144728, average=False, fit_intercept = True, loss='squared_hinge', tol = 0.001573909184776253)\n",
    "X, y = getDataset(1457)\n",
    "sklearn.model_selection.cross_validate(inst, X, y)[\"test_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sklearn = pd.DataFrame(rows, columns=[\"openmlid\", \"seed\", \"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openmlid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.093103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1468</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.096552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.096552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.089655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.072414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4135</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4534</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.093103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4541</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.089655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23512</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.082759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23517</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.096552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40498</th>\n",
       "      <td>1.75</td>\n",
       "      <td>0.073276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40670</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.093103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40685</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40701</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.072414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40900</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40978</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.093103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40981</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.082759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40982</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.093103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40983</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.082759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40984</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.079310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40996</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41027</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41161</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.086207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41162</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.051724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42732</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.089655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42733</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.093103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42734</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.089655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          seed     score\n",
       "openmlid                \n",
       "1111      2.00  0.093103\n",
       "1457      2.00  0.086207\n",
       "1468      2.00  0.086207\n",
       "1475      2.00  0.096552\n",
       "1486      2.00  0.096552\n",
       "1489      2.00  0.086207\n",
       "1515      2.00  0.089655\n",
       "1590      2.00  0.072414\n",
       "4134      2.00  0.086207\n",
       "4135      2.00  0.086207\n",
       "4534      2.00  0.093103\n",
       "4538      2.00  0.086207\n",
       "4541      2.00  0.089655\n",
       "23512     2.00  0.082759\n",
       "23517     2.00  0.096552\n",
       "40498     1.75  0.073276\n",
       "40670     2.00  0.093103\n",
       "40685     2.00  0.086207\n",
       "40701     2.00  0.072414\n",
       "40900     2.00  0.086207\n",
       "40978     2.00  0.093103\n",
       "40981     2.00  0.082759\n",
       "40982     2.00  0.093103\n",
       "40983     2.00  0.082759\n",
       "40984     2.00  0.079310\n",
       "40996     2.00  0.086207\n",
       "41027     2.00  0.086207\n",
       "41161     2.00  0.086207\n",
       "41162     1.00  0.051724\n",
       "42732     2.00  0.089655\n",
       "42733     2.00  0.093103\n",
       "42734     2.00  0.089655"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sklearn.groupby(\"openmlid\").agg(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8879310344827586"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([0.9482758620689655, 0.9310344827586207, 0.9310344827586207, 0.8620689655172413, 0.8620689655172413, 0.8620689655172413, 0.8793103448275862, 0.9310344827586207, 0.8448275862068966, 0.8275862068965517])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
